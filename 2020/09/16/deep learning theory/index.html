<!doctype html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="study,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta name="keywords" content="study">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep learning theory">
<meta property="og:url" content="http://yoursite.com/2020/09/16/deep learning theory/index.html">
<meta property="og:site_name" content="Kehui&#39;s Blog">
<meta property="og:description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://i.imgur.com/ubBUSYs.png">
<meta property="og:image" content="https://i.imgur.com/eAeLPxX.png">
<meta property="og:image" content="https://i.imgur.com/9Ab1jtL.png">
<meta property="og:image" content="https://i.imgur.com/r5qxRdb.png">
<meta property="og:image" content="https://i.imgur.com/hvl7wqt.png">
<meta property="og:image" content="https://i.imgur.com/qpy4iZQ.png">
<meta property="og:image" content="https://i.imgur.com/OqfxHzW.png">
<meta property="og:image" content="https://i.imgur.com/8num3Vw.png">
<meta property="og:image" content="https://i.imgur.com/QM84pRT.png">
<meta property="og:image" content="https://i.imgur.com/Wc2badA.png">
<meta property="og:image" content="https://i.imgur.com/nPWX2Lk.png">
<meta property="og:image" content="https://i.imgur.com/MlVoKPQ.png">
<meta property="og:image" content="https://i.imgur.com/qdZv890.png">
<meta property="og:image" content="https://i.imgur.com/A1eQAwq.png">
<meta property="og:updated_time" content="2020-09-16T19:05:34.450Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep learning theory">
<meta name="twitter:description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta name="twitter:image" content="https://i.imgur.com/ubBUSYs.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/09/16/deep learning theory/">





  <title> Deep learning theory | Kehui's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kehui's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For Kevin Durant</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/16/deep learning theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kehui Yao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kehui's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Deep learning theory
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-16T00:00:00-05:00">
                2020-09-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-09-16T14:05:34-05:00">
                2020-09-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Numerical-computation"><a href="#Numerical-computation" class="headerlink" title="Numerical computation"></a>Numerical computation</h3><h4 id="Directional-derivative"><a href="#Directional-derivative" class="headerlink" title="Directional derivative"></a>Directional derivative</h4><ul>
<li>The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $f(x+\alpha\mu)$ with respect to $\alpha$, evaluated at $\alpha = 0$, this is an numerically correct expression, you can verify it by writing the derivative of the function of $\alpha$.</li>
<li>Using the chain rule, we can see <img src="https://i.imgur.com/ubBUSYs.png" alt="image-20200915105658574"></li>
</ul>
<ul>
<li><p>To minimize f, we would like to find the direction in which f decreases the fastest. We can do this by <img src="https://i.imgur.com/eAeLPxX.png" alt="image-20200915105722737"></p>
<p>Where $\theta$ is the angle between $\mu$ and the gradient, substituting in <img src="https://i.imgur.com/9Ab1jtL.png" alt="image-20200915105830687"> and ignoring factors that do not depend on $\mu$, this simplifies to <img src="https://i.imgur.com/r5qxRdb.png" alt="image-20200915105937469"> This is minimized when $\mu$ points in the opposite direction as the gradient.</p>
</li>
<li><p>This is known as the method of steepest descend , or gradient descent.</p>
</li>
<li><p>Steepest descent proposes a new point <img src="https://i.imgur.com/hvl7wqt.png" alt="image-20200915112118999"></p>
</li>
</ul>
<a id="more"></a>
<h4 id="Choose-step-size-in-different-ways"><a href="#Choose-step-size-in-different-ways" class="headerlink" title="Choose step  size in different ways"></a>Choose step  size in different ways</h4><ul>
<li>Set $\epsilon$ to a small constant.</li>
<li>Evaluate <img src="https://i.imgur.com/qpy4iZQ.png" alt="image-20200915112408672"> for several values of $\epsilon$ and choose the one that results in the smallest objective function value.</li>
<li>Line search.</li>
</ul>
<h3 id="Beyond-the-gradient-Jacobian-and-Hessian-Matrices"><a href="#Beyond-the-gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="Beyond the gradient: Jacobian and Hessian Matrices"></a>Beyond the gradient: Jacobian and Hessian Matrices</h3><ul>
<li><p>Second derivative measures curvature.</p>
</li>
<li><p>If the gradient is 1, then we can make a step of size $\epsilon$ along the negative gradient, and the cost function will decrease by more than $\epsilon$ if the second derivative is negative, meaning that the function curves downward. </p>
</li>
<li><p>The hessian matrix H(f)(x) is defined such that:</p>
<p><img src="https://i.imgur.com/OqfxHzW.png" alt="image-20200915113132517"></p>
<p>Because the hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. </p>
</li>
<li><p>The second derivative in a specific direction represented by a unit vector d is given by $d^{T}Hd$. This  can be easily verified by taking the second derivative of <img src="https://i.imgur.com/8num3Vw.png" alt="image-20200915115859689"> and evaluate at $\alpha = 0$.  </p>
</li>
<li><p>When d is an eigenvector of H, the second derivative in that direction is given by the corresponding eigenvalue. This can be verified by the definition of eigenvalue Ax = $\lambda$x. </p>
</li>
<li><p>For other directions of $d$, the directional second derivative is a weighted average of all the eigenvalues, with weights between 0 and 1, and eigenvectors that have a smaller angle with d receiving more weight. This can be verified by the decomposition of H times unit vector d.</p>
</li>
<li><p>The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalues determine the minimum second derivative. </p>
</li>
<li><p>To summary, the second derivative tells us how well we can expect a gradient descent step to perform.</p>
</li>
<li><p>Second- order Taylor series approximation to the function f(x) around the current point $x^{(0)}$:</p>
<p><img src="https://i.imgur.com/QM84pRT.png" alt="image-20200915120921881"></p>
<p>Where g is the gradient and H is the Hessian.</p>
</li>
<li><p>If we use a learning rate of $\epsilon$, then the new point x will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain <img src="https://i.imgur.com/Wc2badA.png" alt="image-20200915133101857"> There are three terms here: the original value of the function, the expected improvement due to the slope of the function, and the correction we must apply to account for the curvature of the function. When this last term is too large, the gradient descent step can actually move uphill. When $g^{T}Hg$ is zero or negative, the Taylor series approximation predicts that increasing $\epsilon$ forever will decrease f forever.</p>
</li>
<li><p>In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$. So one must resort to more heuristic choices of $\epsilon$ in this case. When $g^{T}Hg$ is positive, solving for the optimal step size that decreases the Taylor series yields <img src="https://i.imgur.com/nPWX2Lk.png" alt="image-20200915134150977"> In the worst case, when g aligns with the eigenvector of H corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $1/\lambda_{max}$. The eigenvalues of the Hessian thus determine the scale of the learning rate.</p>
</li>
<li><p>Second derivative can be used to determine whether a  critical point is a local maximum, a local minimum, or a saddle point. In univariate case, it’s trivial, we can draw a a picture of $x^2$ or $-x^2$ to help us understand. </p>
<p>In multiple dimensions, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum or a saddle point. </p>
<p>When the Hessian matrix is positive definite  (all its eigenvalues are positive), the point is local maximum. This can be seen by observing that the directional second derivative in any directional must be positive, and making reference to the univariate case.</p>
<p>When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that x is a local maximum on one cross section of f but a local minimum on another cross section. </p>
<p><img src="https://i.imgur.com/MlVoKPQ.png" alt="image-20200916110206336"></p>
<p>The test is inconclusive whenever al the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.</p>
</li>
</ul>
<h4 id="Condition-number-of-the-Hessian-matrix"><a href="#Condition-number-of-the-Hessian-matrix" class="headerlink" title="Condition number of the Hessian matrix"></a>Condition number of the Hessian matrix</h4><ul>
<li><p>In multiple dimensions, there is a different second derivative for each direction at a single point.</p>
</li>
<li><p>The condition number of the Hessian at this point measures how much the second derivatives differ from each other. </p>
</li>
<li><p>When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative. So it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.</p>
</li>
<li><p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.</p>
<p><img src="https://i.imgur.com/qdZv890.png" alt="image-20200916114810380"></p>
<p>通俗的来说，如果在一个点x，一阶导数最大的方向恰好是curvature比较大的方向，那么gradient descent会让你往那个方向走。但由于curvature太大，走了一小步一阶导数就就直接反向了，又得往回走。</p>
</li>
</ul>
<h4 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h4><ul>
<li><p>We need to solve this condition number problem.</p>
</li>
<li><p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method is Newton’s method.</p>
</li>
<li><p>The Newton’s method is based on using a second-order Tyaylor series expansion to approximate f(x) near some point $x_{(0)}$:</p>
<p><img src="https://i.imgur.com/A1eQAwq.png" alt="image-20200916115727114"></p>
</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/study/" rel="tag"># study</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/09/13/短途旅行/" rel="next" title="短途旅行">
                <i class="fa fa-chevron-left"></i> 短途旅行
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/16/Statistical methods for epidemiology/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpg" alt="Kehui Yao">
          <p class="site-author-name" itemprop="name">Kehui Yao</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/KEHUIYAO" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/kehui-yao-a5b770165/" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/KehuiY" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-computation"><span class="nav-number">1.</span> <span class="nav-text">Numerical computation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Directional-derivative"><span class="nav-number">1.1.</span> <span class="nav-text">Directional derivative</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choose-step-size-in-different-ways"><span class="nav-number">1.2.</span> <span class="nav-text">Choose step  size in different ways</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beyond-the-gradient-Jacobian-and-Hessian-Matrices"><span class="nav-number">2.</span> <span class="nav-text">Beyond the gradient: Jacobian and Hessian Matrices</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Condition-number-of-the-Hessian-matrix"><span class="nav-number">2.1.</span> <span class="nav-text">Condition number of the Hessian matrix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Newton’s-method"><span class="nav-number">2.2.</span> <span class="nav-text">Newton’s method</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kehui Yao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
