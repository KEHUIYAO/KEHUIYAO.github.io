<!doctype html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="study,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta name="keywords" content="study">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep learning theory">
<meta property="og:url" content="http://yoursite.com/2020/09/16/deep learning theory/index.html">
<meta property="og:site_name" content="Kehui&#39;s Blog">
<meta property="og:description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://i.imgur.com/ubBUSYs.png">
<meta property="og:image" content="https://i.imgur.com/eAeLPxX.png">
<meta property="og:image" content="https://i.imgur.com/9Ab1jtL.png">
<meta property="og:image" content="https://i.imgur.com/r5qxRdb.png">
<meta property="og:image" content="https://i.imgur.com/hvl7wqt.png">
<meta property="og:image" content="https://i.imgur.com/qpy4iZQ.png">
<meta property="og:image" content="https://i.imgur.com/OqfxHzW.png">
<meta property="og:image" content="https://i.imgur.com/8num3Vw.png">
<meta property="og:image" content="https://i.imgur.com/QM84pRT.png">
<meta property="og:image" content="https://i.imgur.com/Wc2badA.png">
<meta property="og:image" content="https://i.imgur.com/nPWX2Lk.png">
<meta property="og:image" content="https://i.imgur.com/MlVoKPQ.png">
<meta property="og:image" content="https://i.imgur.com/qdZv890.png">
<meta property="og:image" content="https://i.imgur.com/NLZHBZk.png">
<meta property="og:image" content="https://i.imgur.com/A1eQAwq.png">
<meta property="og:image" content="https://i.imgur.com/BkARyi6.png">
<meta property="og:image" content="https://i.imgur.com/aHKcHZg.png">
<meta property="og:image" content="https://i.imgur.com/3IgoOCn.png">
<meta property="og:image" content="https://i.imgur.com/vyGaM6o.png">
<meta property="og:image" content="https://i.imgur.com/BpVJKMQ.png">
<meta property="og:image" content="https://i.imgur.com/CDvIj8A.png">
<meta property="og:image" content="https://i.imgur.com/XYf9KAN.png">
<meta property="og:image" content="https://i.imgur.com/MQplF0u.png">
<meta property="og:image" content="https://i.imgur.com/jSb38dP.png">
<meta property="og:image" content="https://i.imgur.com/TXGr64K.png">
<meta property="og:image" content="https://i.imgur.com/7pUMGt9.png">
<meta property="og:image" content="https://i.imgur.com/8lsEvGy.png">
<meta property="og:image" content="https://i.imgur.com/ZWRuLqK.png">
<meta property="og:image" content="https://i.imgur.com/BlVVJzw.png">
<meta property="og:image" content="https://i.imgur.com/Y28pzUD.png">
<meta property="og:image" content="https://i.imgur.com/EMFXt0y.png">
<meta property="og:image" content="https://i.imgur.com/yP3WpaM.png">
<meta property="og:image" content="https://i.imgur.com/jDKOXvR.png">
<meta property="og:image" content="https://i.imgur.com/fUFg67t.png">
<meta property="og:image" content="https://i.imgur.com/QQxfqWj.png">
<meta property="og:image" content="https://i.imgur.com/lhhFzxz.png">
<meta property="og:image" content="https://i.imgur.com/89aT2oI.png">
<meta property="og:image" content="https://i.imgur.com/qPoOVuT.png">
<meta property="og:image" content="https://i.imgur.com/Rz7ILoI.png">
<meta property="og:image" content="https://i.imgur.com/kGNg0Xz.png">
<meta property="og:image" content="https://i.imgur.com/4if2LY1.png">
<meta property="og:image" content="https://i.imgur.com/exANV8l.png">
<meta property="og:image" content="https://i.imgur.com/g5lwFVR.png">
<meta property="og:image" content="https://i.imgur.com/x4GTTeB.png">
<meta property="og:image" content="https://i.imgur.com/JJrWZLD.png">
<meta property="og:image" content="https://i.imgur.com/4Ou8ok9.png">
<meta property="og:image" content="https://i.imgur.com/YlTtlHl.png">
<meta property="og:image" content="https://i.imgur.com/2P883IF.png">
<meta property="og:image" content="https://i.imgur.com/lXHwBgz.png">
<meta property="og:image" content="https://i.imgur.com/p7STyI9.png">
<meta property="og:image" content="https://i.imgur.com/wnm0Fe5.png">
<meta property="og:image" content="https://i.imgur.com/iYwBFPy.png">
<meta property="og:image" content="https://i.imgur.com/fmj0gfv.png">
<meta property="og:image" content="https://i.imgur.com/QaeIWpx.png">
<meta property="og:image" content="https://i.imgur.com/eIXw2Y9.png">
<meta property="og:image" content="https://i.imgur.com/lOACxn9.png">
<meta property="og:image" content="https://i.imgur.com/LNCucfD.png">
<meta property="og:image" content="https://i.imgur.com/sPcQhSt.png">
<meta property="og:image" content="https://i.imgur.com/RG4CQJq.png">
<meta property="og:updated_time" content="2020-10-09T03:39:25.066Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep learning theory">
<meta name="twitter:description" content="Numerical computationDirectional derivative The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $">
<meta name="twitter:image" content="https://i.imgur.com/ubBUSYs.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/09/16/deep learning theory/">





  <title> Deep learning theory | Kehui's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kehui's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For Kevin Durant</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/16/deep learning theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kehui Yao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kehui's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Deep learning theory
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-16T00:00:00-05:00">
                2020-09-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-10-08T22:39:25-05:00">
                2020-10-08
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Numerical-computation"><a href="#Numerical-computation" class="headerlink" title="Numerical computation"></a>Numerical computation</h3><h4 id="Directional-derivative"><a href="#Directional-derivative" class="headerlink" title="Directional derivative"></a>Directional derivative</h4><ul>
<li>The directional derivative in direction $\mu$ (a unit vector) is the slope of he function f in direction u. Formally, it is the derivative of the function $f(x+\alpha\mu)$ with respect to $\alpha$, evaluated at $\alpha = 0$, this is an numerically correct expression, you can verify it by writing the derivative of the function of $\alpha$.</li>
<li>Using the chain rule, we can see <img src="https://i.imgur.com/ubBUSYs.png" alt="image-20200915105658574"></li>
</ul>
<ul>
<li><p>To minimize f, we would like to find the direction in which f decreases the fastest. We can do this by <img src="https://i.imgur.com/eAeLPxX.png" alt="image-20200915105722737"></p>
<p>Where $\theta$ is the angle between $\mu$ and the gradient, substituting in <img src="https://i.imgur.com/9Ab1jtL.png" alt="image-20200915105830687"> and ignoring factors that do not depend on $\mu$, this simplifies to <img src="https://i.imgur.com/r5qxRdb.png" alt="image-20200915105937469"> This is minimized when $\mu$ points in the opposite direction as the gradient.</p>
</li>
<li><p>This is known as the method of steepest descend , or gradient descent.</p>
</li>
<li><p>Steepest descent proposes a new point <img src="https://i.imgur.com/hvl7wqt.png" alt="image-20200915112118999"></p>
</li>
</ul>
<a id="more"></a>
<h4 id="Choose-step-size-in-different-ways"><a href="#Choose-step-size-in-different-ways" class="headerlink" title="Choose step  size in different ways"></a>Choose step  size in different ways</h4><ul>
<li>Set $\epsilon$ to a small constant.</li>
<li>Evaluate <img src="https://i.imgur.com/qpy4iZQ.png" alt="image-20200915112408672"> for several values of $\epsilon$ and choose the one that results in the smallest objective function value.</li>
<li>Line search.</li>
</ul>
<h3 id="Beyond-the-gradient-Jacobian-and-Hessian-Matrices"><a href="#Beyond-the-gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="Beyond the gradient: Jacobian and Hessian Matrices"></a>Beyond the gradient: Jacobian and Hessian Matrices</h3><ul>
<li><p>Second derivative measures curvature.</p>
</li>
<li><p>If the gradient is 1, then we can make a step of size $\epsilon$ along the negative gradient, and the cost function will decrease by more than $\epsilon$ if the second derivative is negative, meaning that the function curves downward. </p>
</li>
<li><p>The hessian matrix H(f)(x) is defined such that:</p>
<p><img src="https://i.imgur.com/OqfxHzW.png" alt="image-20200915113132517"></p>
<p>Because the hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. </p>
</li>
<li><p>The second derivative in a specific direction represented by a unit vector d is given by $d^{T}Hd$. This  can be easily verified by taking the second derivative of <img src="https://i.imgur.com/8num3Vw.png" alt="image-20200915115859689"> and evaluate at $\alpha = 0$.  </p>
</li>
<li><p>When d is an eigenvector of H, the second derivative in that direction is given by the corresponding eigenvalue. This can be verified by the definition of eigenvalue Ax = $\lambda$x. </p>
</li>
<li><p>For other directions of $d$, the directional second derivative is a weighted average of all the eigenvalues, with weights between 0 and 1, and eigenvectors that have a smaller angle with d receiving more weight. This can be verified by the decomposition of H times unit vector d.</p>
</li>
<li><p>The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalues determine the minimum second derivative. </p>
</li>
<li><p>To summary, the second derivative tells us how well we can expect a gradient descent step to perform.</p>
</li>
<li><p>Second- order Taylor series approximation to the function f(x) around the current point $x^{(0)}$:</p>
<p><img src="https://i.imgur.com/QM84pRT.png" alt="image-20200915120921881"></p>
<p>Where g is the gradient and H is the Hessian.</p>
</li>
<li><p>If we use a learning rate of $\epsilon$, then the new point x will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain <img src="https://i.imgur.com/Wc2badA.png" alt="image-20200915133101857"> There are three terms here: the original value of the function, the expected improvement due to the slope of the function, and the correction we must apply to account for the curvature of the function. When this last term is too large, the gradient descent step can actually move uphill. When $g^{T}Hg$ is zero or negative, the Taylor series approximation predicts that increasing $\epsilon$ forever will decrease f forever.</p>
</li>
<li><p>In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$. So one must resort to more heuristic choices of $\epsilon$ in this case. When $g^{T}Hg$ is positive, solving for the optimal step size that decreases the Taylor series yields <img src="https://i.imgur.com/nPWX2Lk.png" alt="image-20200915134150977"> In the worst case, when g aligns with the eigenvector of H corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $1/\lambda_{max}$. The eigenvalues of the Hessian thus determine the scale of the learning rate.</p>
</li>
<li><p>Second derivative can be used to determine whether a  critical point is a local maximum, a local minimum, or a saddle point. In univariate case, it’s trivial, we can draw a a picture of $x^2$ or $-x^2$ to help us understand. </p>
<p>In multiple dimensions, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum or a saddle point. </p>
<p>When the Hessian matrix is positive definite  (all its eigenvalues are positive), the point is local maximum. This can be seen by observing that the directional second derivative in any directional must be positive, and making reference to the univariate case.</p>
<p>When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that x is a local maximum on one cross section of f but a local minimum on another cross section. </p>
<p><img src="https://i.imgur.com/MlVoKPQ.png" alt="image-20200916110206336"></p>
<p>The test is inconclusive whenever al the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.</p>
</li>
</ul>
<h4 id="Condition-number-of-the-Hessian-matrix"><a href="#Condition-number-of-the-Hessian-matrix" class="headerlink" title="Condition number of the Hessian matrix"></a>Condition number of the Hessian matrix</h4><ul>
<li><p>In multiple dimensions, there is a different second derivative for each direction at a single point.</p>
</li>
<li><p>The condition number of the Hessian at this point measures how much the second derivatives differ from each other. </p>
</li>
<li><p>When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative. So it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.</p>
</li>
<li><p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.</p>
<p><img src="https://i.imgur.com/qdZv890.png" alt="image-20200916114810380"></p>
<p>刚开始的时候step size还挺大的，后面由于curvature变得越来越大，步长就越来越小了。<img src="https://i.imgur.com/NLZHBZk.png" alt="image-20200919103543879"></p>
</li>
</ul>
<h4 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h4><ul>
<li><p>We need to solve this condition number problem.</p>
</li>
<li><p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method is Newton’s method.</p>
</li>
<li><p>The Newton’s method is based on using a second-order Tyaylor series expansion to approximate f(x) near some point $x_{(0)}$:</p>
<p><img src="https://i.imgur.com/A1eQAwq.png" alt="image-20200916115727114"></p>
<p>If we then solve for the critical point of this function, we obtain </p>
<p><img src="https://i.imgur.com/BkARyi6.png" alt="image-20200918232903625"></p>
</li>
</ul>
<h4 id="Numerical-Precision-a-deep-earning-super-kill"><a href="#Numerical-Precision-a-deep-earning-super-kill" class="headerlink" title="Numerical Precision: a deep earning super kill"></a>Numerical Precision: a deep earning super kill</h4><ul>
<li><p>Often deep learning algorithms “sort of work”</p>
</li>
<li><p>Often deep learning algorithms “explode”</p>
</li>
<li><p>Culprit is often loss of numerical precision</p>
</li>
<li><p>Rounding and truncation errors:</p>
<ul>
<li>In a digital computer, we use float32 or similar schemes to represent real numbers.</li>
<li>A real number x is rounded to x+delta for some small delta.</li>
<li>Overflow: large x replaced by inf</li>
<li>Underflow: small x replaced by 0 (very harmful)</li>
</ul>
</li>
<li><p><img src="https://i.imgur.com/aHKcHZg.png" alt="image-20200919104747621"></p>
</li>
<li><p><img src="https://i.imgur.com/3IgoOCn.png" alt="image-20200919104905561"></p>
</li>
<li><p>Exp function:</p>
<ul>
<li>Overflows for large x</li>
<li>Doesn’t need to be very large</li>
<li>Float32:89 overflows</li>
<li>Never use large x</li>
<li>Underflows for very negative x, possibly catastrophic if exp(x) in denominator.</li>
</ul>
</li>
<li><p>Subtraction:</p>
<p><img src="https://i.imgur.com/vyGaM6o.png" alt="image-20200919105202962"></p>
</li>
<li><p>Log and sqrt:</p>
<p><img src="https://i.imgur.com/BpVJKMQ.png" alt="image-20200919105311566"></p>
</li>
</ul>
<ul>
<li><p>Log exp:</p>
<p><img src="https://i.imgur.com/CDvIj8A.png" alt="image-20200919105456570"></p>
</li>
<li><p>Which is the better hack?</p>
<p><img src="https://i.imgur.com/XYf9KAN.png" alt="image-20200919105601015"></p>
<p>The first one is better, because it avoids the derivative of sqrt(x) equals to 0.</p>
</li>
<li><p>Log(sum(exp))</p>
<p><img src="https://i.imgur.com/MQplF0u.png" alt="image-20200919105928959"></p>
<p><img src="https://i.imgur.com/jSb38dP.png" alt="image-20200919110029920"></p>
<p>Why does the logsumexp trick work?</p>
<p>Algebraically equivalent to  the original version:</p>
<p><img src="https://i.imgur.com/TXGr64K.png" alt="image-20200919114807312"></p>
<p>Miss one log in the last equation……</p>
<p><img src="https://i.imgur.com/7pUMGt9.png" alt="image-20200919114938929"></p>
</li>
<li><p>Softmax</p>
<p><img src="https://i.imgur.com/8lsEvGy.png" alt="image-20200919115034258"></p>
</li>
</ul>
<ul>
<li><p>Sigmoid</p>
<p><img src="https://i.imgur.com/ZWRuLqK.png" alt="image-20200919115202817"></p>
</li>
</ul>
<ul>
<li><p>cross-entropy</p>
<p><img src="https://i.imgur.com/BlVVJzw.png" alt="image-20200919223037263"></p>
<p>Recommendations are using built-in library.</p>
</li>
<li><p>Bug hunting strategies:</p>
<ul>
<li><img src="https://i.imgur.com/Y28pzUD.png" alt="image-20200919223239296"></li>
</ul>
</li>
</ul>
<ul>
<li><img src="https://i.imgur.com/EMFXt0y.png" alt="image-20200919223440481"></li>
</ul>
<h3 id="Machine-learning-basics"><a href="#Machine-learning-basics" class="headerlink" title="Machine learning basics"></a>Machine learning basics</h3><h4 id="Different-tasks-of-machine-learning"><a href="#Different-tasks-of-machine-learning" class="headerlink" title="Different tasks of machine learning"></a>Different tasks of machine learning</h4><ul>
<li><p>Classification with missing inputs</p>
<ul>
<li>Learn a set of functions, each function corresponds to classifying x with a different subset of its inputs missing.</li>
<li>A more efficient way is to learn a probability distribution over all the relevant variables, then solve the classification task by marginalizing out the missing variables.</li>
</ul>
</li>
<li><p>Transcription</p>
<ul>
<li>Optical character recognition</li>
<li>Google street view uses deep learning to process address numbers</li>
<li>Speech recognition</li>
</ul>
</li>
<li>Machine translation<ul>
<li>A sequence of symbols in some language converted to a sequence of symbols in another language</li>
</ul>
</li>
<li>Structured output<ul>
<li>The output values should be tightly interrelated, examples are as follows</li>
<li>Mapping a natural language sentence into a tree that describes its grammatical structure by tagging nodes of the trees as being verbs, nouns, adverbs and so on</li>
<li>Image captioning, the words produced by an image captioning must form a valid sentence</li>
</ul>
</li>
<li>Anomaly detection<ul>
<li>Credit card fraud detection</li>
</ul>
</li>
<li>Synthesis and sampling<ul>
<li>The machine is asked to generate new examples that are similar to those in the training data </li>
<li>Video games can automatically generate textures for large objects or landscapes, rather than requiring an artist to manually label each pixel.</li>
<li>Given the input, generate the specific kind of output. For example, we provide a written sentence and ask the program to emit an audio waveform containing a spoken version of that sentence. </li>
</ul>
</li>
<li>Denoising<ul>
<li>Predict the clean example x from its corrupted version xhat.</li>
</ul>
</li>
<li>Density estimation <ul>
<li>Learn a function $p_{model}: R^n $ to $R$.</li>
</ul>
</li>
</ul>
<h4 id="The-performance-measure-P"><a href="#The-performance-measure-P" class="headerlink" title="The performance measure P"></a>The performance measure P</h4><ul>
<li>Accuracy: the proportion of examples for which the model produces the correct output</li>
<li>Log-probability the model assigns to some examples</li>
<li>Sometimes hard to choose a performance measure that corresponds well to the desired behavior of the system.<ul>
<li>When doing regression task, should we penalize the system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes</li>
</ul>
</li>
</ul>
<h4 id="The-experience-E"><a href="#The-experience-E" class="headerlink" title="The experience E"></a>The experience E</h4><ul>
<li>Unsupervised learning </li>
<li>Superviesed learning</li>
<li>Reinforcement learning algorithms interacts with an environment, which does not just experience a fixed dataset.</li>
</ul>
<h4 id="Generalization"><a href="#Generalization" class="headerlink" title="Generalization"></a>Generalization</h4><ul>
<li>It requires our algorithm to perform well on new, previously unseen inputs, not just those on which our model was trained.</li>
<li>Test error is generalization error</li>
</ul>
<h4 id="Occam’s-razor"><a href="#Occam’s-razor" class="headerlink" title="Occam’s razor"></a>Occam’s razor</h4><ul>
<li>This principle states that among competing hypotheses that explain known observations equally well, we should choose the “simplest” one. </li>
</ul>
<h4 id="Vapnik-Chervonenkis-dimension"><a href="#Vapnik-Chervonenkis-dimension" class="headerlink" title="Vapnik-Chervonenkis dimension"></a>Vapnik-Chervonenkis dimension</h4><ul>
<li>Measure the capacity of a binary classifier.</li>
<li>Defined as  being the largest possible value of m for which there exists a training set of m different x points that the classifier can label arbitrarily.</li>
</ul>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><ul>
<li>Expressing preferences for one function over another is a more general way of controlling a model’s capacity than including or excluding members from the   hypothesis space.</li>
<li>We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.</li>
</ul>
<h4 id="Validation-set-and-hyperparameters"><a href="#Validation-set-and-hyperparameters" class="headerlink" title="Validation set and hyperparameters"></a>Validation set and hyperparameters</h4><ul>
<li>What is hyperparameters?<ul>
<li>For example, in linear regression, the degree of the polynomial, which acts as a capacity hyperparameter.</li>
</ul>
</li>
<li>Why use validation set?<ul>
<li>Train hyperparameters</li>
</ul>
</li>
<li>Why not train hyperparameters using training data?<ul>
<li>Such hyperparameters would always choose the maximum model capacity, result in overfitting.</li>
</ul>
</li>
</ul>
<h4 id="Statistics-theory-basic"><a href="#Statistics-theory-basic" class="headerlink" title="Statistics theory basic"></a>Statistics theory basic</h4><ul>
<li>One can see my another blog: statistics theory</li>
</ul>
<h4 id="Maximum-likelihood-estimation"><a href="#Maximum-likelihood-estimation" class="headerlink" title="Maximum likelihood estimation"></a>Maximum likelihood estimation</h4><ul>
<li><p><img src="https://i.imgur.com/yP3WpaM.png" alt="image-20200922012615930"></p>
</li>
<li><p>Because rescale the cost function does not change the argmax.</p>
<p><img src="https://i.imgur.com/jDKOXvR.png" alt="image-20200922012653791"></p>
</li>
<li><p>One way to interpret mle is to view it as minimizing the dissimilarity between the empirical distribution pdata and the model distribution</p>
</li>
<li><p>The dissimilarity between the two can be measured by the KL divergence. </p>
<p><img src="https://i.imgur.com/fUFg67t.png" alt="image-20200922012943236"></p>
</li>
<li><p>Cross entropy:</p>
<p>Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.</p>
</li>
</ul>
<h4 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h4><ul>
<li>Idea: gradient descent requires computing <img src="https://i.imgur.com/QQxfqWj.png" alt="image-20200923095906722">, as the training size grows larger, the time to take a single gradient step becomes long. The insight of SGD is to estimate the expectation using a small set of samples.</li>
<li>On each step, sample a mini batch uniformly from the training set. And the estimate of the gradient is formed as <img src="https://i.imgur.com/lhhFzxz.png" alt="image-20200923100152249"> </li>
</ul>
<h4 id="Challenges-motivating-deep-learning"><a href="#Challenges-motivating-deep-learning" class="headerlink" title="Challenges motivating deep learning"></a>Challenges motivating deep learning</h4><ul>
<li>The curse of dimensionality:<ul>
<li>In one dimension, when we generalize to a new data point, we usually tel what to do  simply by inspecting the training examples that lie in the same cell as the new input</li>
<li>When dimension is high, sometimes we cannot find the training data located in that space.</li>
<li>We can usually suppose we have a variable taking value of 0 and 1. If we have 100 variables, we need at least 2^100 training data points to have a whole picture of the space.</li>
</ul>
</li>
</ul>
<h4 id="Local-constancy-and-smoothness-regularization"><a href="#Local-constancy-and-smoothness-regularization" class="headerlink" title="Local constancy and smoothness regularization"></a>Local constancy and smoothness regularization</h4><ul>
<li><p>Smoothness prior: </p>
<p>This prior states that the function we learn should not change very much within a small region</p>
</li>
<li><p>Many simpler algorithms rely exclusively on this prior to generalize well, and as a result, they fail to scale to the statistical challenges involved in solving AI-level tasks.</p>
</li>
<li><p>K-nearest neighbors family of learning algorithms is an extreme example of the local constancy approach.</p>
</li>
<li><p>Decision trees also suffer from the limitations of exclusively smoothness-based learning, because:</p>
<ul>
<li>It break the input space into as many regions as there are leaves and use a separate (or more) </li>
<li>If a target function requires a tree with at least n leaves to be represented accurately, then at least n training examples are required to fit the tree.</li>
</ul>
</li>
<li><p>If the function is complicated (we want to distinguish a huge number of regions compared to the number of examples), is there any hope to generalize well?</p>
<ul>
<li>Answer is yes</li>
<li>The key insight is that a very large number of regions, such as $O(2^k)$, can be defined with $O(k)$ examples, so long as we introduce some dependencies between the regions through additional assumptions about the underlying data-generating distribution. </li>
<li>In this way, we can generalize nonlocally.</li>
<li>Many different deep learning algorithms provide implicit or explicit assumptions of nonlocality.</li>
</ul>
</li>
</ul>
<h4 id="Manifold-learning"><a href="#Manifold-learning" class="headerlink" title="Manifold learning"></a>Manifold learning</h4><ul>
<li><p>A very nice answer on zhihu: <a href="https://www.zhihu.com/question/24015486" target="_blank" rel="noopener">https://www.zhihu.com/question/24015486</a></p>
</li>
<li><p>Essentially is Nonlinear dimension reduction.</p>
<p><img src="https://i.imgur.com/89aT2oI.png" alt="image-20200923152423094"></p>
</li>
</ul>
<h3 id="Deep-Feedforward-Networks"><a href="#Deep-Feedforward-Networks" class="headerlink" title="Deep Feedforward Networks"></a>Deep Feedforward Networks</h3><h4 id="Computational-graph-and-backpropagation"><a href="#Computational-graph-and-backpropagation" class="headerlink" title="Computational graph and backpropagation"></a>Computational graph and backpropagation</h4><ul>
<li>博客资料 <a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Backprop/</a></li>
<li><p>视频资料 <a href="https://www.youtube.com/watch?v=-yhm3WdGFok" target="_blank" rel="noopener">https://www.youtube.com/watch?v=-yhm3WdGFok</a></p>
</li>
<li><p>正向传播走一遍算出所有节点的值</p>
</li>
<li>reverse mode走一遍，存下cost对所有node的偏微分</li>
</ul>
<h4 id="Learn-conditional-statistics"><a href="#Learn-conditional-statistics" class="headerlink" title="Learn conditional statistics"></a>Learn conditional statistics</h4><p>When we use cross-entropy loss, we are learning p(y|x), actually we only need to learn one conditional statistics of y given x, say E(y|x).</p>
<ul>
<li><p><img src="https://i.imgur.com/qPoOVuT.png" alt="image-20200926202307454"></p>
<p>if we are using the square loss, we can replace f(x) by E(y|x) and train the model using gradient descent</p>
</li>
<li><p>Here we can use we only need to use the information of E(y|x), not f(y|x), seemed that we are saving some resources.</p>
</li>
<li><p>The truth is, the cost function in this way is hard to train. Some output units that saturate produce very small gradients when combined with these cost functions.</p>
</li>
</ul>
<h4 id="Sigmoid-units-for-Bernoulli-Output-distribution"><a href="#Sigmoid-units-for-Bernoulli-Output-distribution" class="headerlink" title="Sigmoid units for Bernoulli Output distribution:"></a>Sigmoid units for Bernoulli Output distribution:</h4><ul>
<li><p>Intuition of define a probability over y using values of z:</p>
<ul>
<li><p>we first obtain an unnormalized probability by assuming that log(p(y|z)) = yz, the left hand side can take value from -inf to inf since unnormalized p(y|z)&gt;0 , and yz takes value from -inf to inf, so this assumption is reasonable.</p>
</li>
<li><p>Another reason of choosing this assumption is to undo the log under cross entropy.</p>
</li>
<li><p>So:</p>
<p><img src="https://i.imgur.com/Rz7ILoI.png" alt="image-20200926204835485"></p>
<p><img src="https://i.imgur.com/kGNg0Xz.png" alt="image-20200926204857106"></p>
</li>
<li><p>The symbol appeared in the last equation is called the softplus. </p>
</li>
<li><p><img src="https://i.imgur.com/4if2LY1.png" alt="image-20200926205249603"></p>
</li>
</ul>
</li>
</ul>
<h4 id="Softmax-units-for-multinoulli-output-distributions"><a href="#Softmax-units-for-multinoulli-output-distributions" class="headerlink" title="Softmax units for multinoulli output distributions"></a>Softmax units for multinoulli output distributions</h4><p><img src="https://i.imgur.com/exANV8l.png" alt="image-20200926213815126"></p>
<p><img src="https://i.imgur.com/g5lwFVR.png" alt="image-20200926213827105"></p>
<h3 id="Regularization-for-deep-learning"><a href="#Regularization-for-deep-learning" class="headerlink" title="Regularization for deep learning"></a>Regularization for deep learning</h3><h4 id="L2-normalization"><a href="#L2-normalization" class="headerlink" title="L2 normalization"></a>L2 normalization</h4><ul>
<li><p>Aka weight decay</p>
</li>
<li><p>Consider a model has the following total objective function:</p>
<p><img src="https://i.imgur.com/x4GTTeB.png" alt="image-20201008001403886"></p>
<p>With the corresponding parameter gradient</p>
<p><img src="https://i.imgur.com/JJrWZLD.png" alt="image-20201008001446527"></p>
<p>To take a single gradient step to update the weights, we perform this update:</p>
<p><img src="https://i.imgur.com/4Ou8ok9.png" alt="image-20201008001523308"></p>
<p>Written another way, the update is:</p>
<p><img src="https://i.imgur.com/YlTtlHl.png" alt="image-20201008001601908"></p>
</li>
</ul>
<h4 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h4><ul>
<li><p>The phenomenon that training objective decreases consistently over time, but the validation set average loss eventually begins to increase again, forming an asymmetric U-shaped curve.</p>
</li>
<li><p><img src="https://i.imgur.com/2P883IF.png" alt="image-20200926225245821"></p>
</li>
<li>consider the training time as a hyperparameter. Early stopping algorithm is to tune this parameter.</li>
<li>do the training process and periodically record the model performance on validation set. If the performance decreases, we stop the training.</li>
<li><img src="https://i.imgur.com/lXHwBgz.png" alt="image-20200926225932268"></li>
<li>early stopping is kind of a regularization method because it mathematically has the same effect as L2 regularizations.</li>
</ul>
<h4 id="parameter-sharing"><a href="#parameter-sharing" class="headerlink" title="parameter sharing"></a>parameter sharing</h4><ul>
<li>from knowledge of the domain and model architecture, that there should be some dependencies between the model parameters</li>
<li>a typical example is CNN</li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><ul>
<li>Dropout can be thought of as a method of making bagging practical for ensembles of very many large neural networks</li>
<li>Intuition: cannot rely on any one feature, so have to spread out weights, resulting in shrinking weights like L2 norm.</li>
<li>Frequently used in computer vision where input is huge.</li>
<li>Dropout trains not just a bagged ensemble of model, but an ensemble of models that share hidden units. This means each hidden unit must be able to perform well regardless of which other hidden units are in the model. Hidden units must be prepared to be swapped and interchanged between models.</li>
<li>The noise is multiplicative. If the noise were additive with fixed scale, then a relu hidden unit $h_i$ with added noise $\epsilon$ could simply learn to have $h_i$ become very large in order to make the added noise $\epsilon$ insignificant by comparison. </li>
<li></li>
</ul>
<h4 id="Sparse-Representations"><a href="#Sparse-Representations" class="headerlink" title="Sparse Representations"></a>Sparse Representations</h4><ul>
<li><p>L1 penalization induces a sparse parametrization, meaning that many of the parameters become zero.</p>
<p><img src="https://i.imgur.com/p7STyI9.png" alt="image-20201007163307987"></p>
</li>
<li><p>Representational sparsity, on the other hand, describes a representation where many of the elements of the representations are zero.</p>
<p><img src="https://i.imgur.com/wnm0Fe5.png" alt="image-20201007163540259"></p>
</li>
<li><p>Just as an L1 penalty on the parameters induces parameter sparsity, an L1 penalty on the elements of the representation induces representational sparsity.</p>
</li>
</ul>
<h4 id="Bagging-and-other-ensemble-methods"><a href="#Bagging-and-other-ensemble-methods" class="headerlink" title="Bagging and other ensemble methods"></a>Bagging and other ensemble methods</h4><ul>
<li><p>Bagging is a technique for reducing generalization error by combining several models. The idea is to train several different models separately, then have all the models vote on the output for the test examples.</p>
</li>
<li><p>Consider for example a set of k regression models. </p>
<p><img src="https://i.imgur.com/iYwBFPy.png" alt="image-20201007211337587"></p>
</li>
</ul>
<h4 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h4><p><a href="https://www.youtube.com/watch?v=CIfsB_EYsVI&amp;t=1348s" target="_blank" rel="noopener">https://www.youtube.com/watch?v=CIfsB_EYsVI&amp;t=1348s</a></p>
<ul>
<li><p>You want to make some data that will have very low performance (fool the network). This is actually an easy task to do.</p>
</li>
<li><p>It is because attacking a linear model is simple.</p>
</li>
<li><p>Modern deep nets are very piecewise linear.</p>
</li>
<li><p>The fast gradient sign method is very popular to generate the adversarial examples:</p>
<ul>
<li><p><img src="https://i.imgur.com/fmj0gfv.png" alt="image-20201006221107960"></p>
<p>这里用到了sign gradient，含义是如果往一个$\delta J(x)/\delta x_i$的值是正的，那么sign()以后的值就为1</p>
</li>
<li><p>Adversarial examples are not noise</p>
</li>
<li><p>Cross-technique transferability allows you to use the adversarial examples developed on your own training algorithms to fool others’ network</p>
</li>
<li><p><img src="https://i.imgur.com/QaeIWpx.png" alt="image-20201006221706173"></p>
</li>
<li><p>If you impose this adversarial attack on the autonomous vehicles, you can fool them to think the passengers as navigable road, which is super-dangerous!</p>
</li>
<li><p>Training on adversarial examples can help.</p>
</li>
<li><p>Another technique is called virtual adversarial training.</p>
<p><img src="https://i.imgur.com/eIXw2Y9.png" alt="image-20201006232114148"></p>
</li>
<li><p>Adversarial training method can also be applied to universal engineering machine. We can make new inventions by finding input that maximizes model’s predicted performance.</p>
</li>
<li><p>Conclusion:</p>
<ul>
<li>Attacking is easy.</li>
<li>Defending is difficult.</li>
</ul>
</li>
<li><p>Open-source library cleverhans:</p>
<p><img src="https://i.imgur.com/lOACxn9.png" alt="image-20201006232543162"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Deep-learning-training-tips"><a href="#Deep-learning-training-tips" class="headerlink" title="Deep learning training tips"></a>Deep learning training tips</h3><h4 id="testing-the-result-on-training-data-set"><a href="#testing-the-result-on-training-data-set" class="headerlink" title="testing the result on training data set"></a>testing the result on training data set</h4><p>deeper does not imply better</p>
<h4 id="Vanishing-gradient-problem"><a href="#Vanishing-gradient-problem" class="headerlink" title="Vanishing gradient problem"></a>Vanishing gradient problem</h4><ul>
<li><p>near input: </p>
<ul>
<li>smaller gradients</li>
<li>learn very slow</li>
<li>almost random</li>
</ul>
</li>
<li><p>near output:</p>
<ul>
<li>larger gradients</li>
<li>learn very fast</li>
<li>already converge</li>
</ul>
</li>
<li><p>use relu, not sigmoid</p>
<ul>
<li>relu: a thinner linear network, do not have smaller gradients</li>
<li>sigmoid: vanishing gradient</li>
<li>some variations of relu: leaky relu, parametric relu, maxout(learnable activation function)</li>
</ul>
</li>
<li>maxout:<ul>
<li>activation function in maxout network can be any piecewise linear convex function</li>
<li>how many pieces depending on how many elements in a group</li>
<li>how to train: train a thin and linear network. Different thin and linear network for different examples</li>
</ul>
</li>
</ul>
<h4 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h4><ul>
<li><p>adagrad:  we give smaller learning rate when the historical gradients are large. 陡峭的地方一直陡峭，平坦的地方一直平坦。</p>
</li>
<li><p>rmsprop: <img src="https://i.imgur.com/LNCucfD.png" alt="image-20200926124712058"></p>
<p>新看到的gradient比较大的weight，过去的gradient给比较小的weight。</p>
</li>
</ul>
<h4 id="Hard-to-find-optimal-network-parameters"><a href="#Hard-to-find-optimal-network-parameters" class="headerlink" title="Hard to find optimal network parameters"></a>Hard to find optimal network parameters</h4><ul>
<li>very slow at the plateau </li>
<li>stuck at saddle point</li>
<li>stuck at local minima</li>
</ul>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><ul>
<li><p>Intuition from the  physical world: how about put momentum phenomenon into gradient descent</p>
</li>
<li><p>Momentum: movement of last step minus gradient at present (惯性)</p>
<p><img src="https://i.imgur.com/sPcQhSt.png" alt="image-20200926171450571"></p>
</li>
<li><p>Movement = negative of gradient + Momentum</p>
</li>
</ul>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Combining rmsprop and momentum</p>
<h4 id="Regularization-1"><a href="#Regularization-1" class="headerlink" title="Regularization"></a>Regularization</h4><ul>
<li>new loss function to be minimized</li>
<li>L1 regularization: always delete</li>
<li>L2 regularization: always scale</li>
</ul>
<h4 id="Early-stopping-1"><a href="#Early-stopping-1" class="headerlink" title="Early stopping"></a>Early stopping</h4><h4 id="Dropout-1"><a href="#Dropout-1" class="headerlink" title="Dropout"></a>Dropout</h4><ul>
<li><p>training: each time before updating the parameter:</p>
<ul>
<li>each neuron has p% to dropout</li>
</ul>
</li>
<li><p>Intuitive reason:</p>
<ul>
<li>when teams up, if everyone expect the partner will do the work, nothing will be done finally.</li>
<li>However, if you know your partner will dropout, you will do better.</li>
<li>When testing, no one dropout actually, so obtaining good results eventually.</li>
</ul>
</li>
<li><p>Why the weights should multiply (1-p)% (dropout rate) when testing?</p>
<ul>
<li><p>Assume dropout rate is 50%:</p>
<p><img src="https://i.imgur.com/RG4CQJq.png" alt="image-20200926191233011"></p>
</li>
</ul>
</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/study/" rel="tag"># study</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/09/16/spatial temporal machine learning/" rel="next" title="Spatial temporal machine learning">
                <i class="fa fa-chevron-left"></i> Spatial temporal machine learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/16/Statistical methods for epidemiology/" rel="prev" title="Statistical methods for epidemiology">
                Statistical methods for epidemiology <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpg" alt="Kehui Yao">
          <p class="site-author-name" itemprop="name">Kehui Yao</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/KEHUIYAO" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/kehui-yao-a5b770165/" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/KehuiY" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-computation"><span class="nav-number">1.</span> <span class="nav-text">Numerical computation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Directional-derivative"><span class="nav-number">1.1.</span> <span class="nav-text">Directional derivative</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choose-step-size-in-different-ways"><span class="nav-number">1.2.</span> <span class="nav-text">Choose step  size in different ways</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beyond-the-gradient-Jacobian-and-Hessian-Matrices"><span class="nav-number">2.</span> <span class="nav-text">Beyond the gradient: Jacobian and Hessian Matrices</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Condition-number-of-the-Hessian-matrix"><span class="nav-number">2.1.</span> <span class="nav-text">Condition number of the Hessian matrix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Newton’s-method"><span class="nav-number">2.2.</span> <span class="nav-text">Newton’s method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Numerical-Precision-a-deep-earning-super-kill"><span class="nav-number">2.3.</span> <span class="nav-text">Numerical Precision: a deep earning super kill</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-learning-basics"><span class="nav-number">3.</span> <span class="nav-text">Machine learning basics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Different-tasks-of-machine-learning"><span class="nav-number">3.1.</span> <span class="nav-text">Different tasks of machine learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-performance-measure-P"><span class="nav-number">3.2.</span> <span class="nav-text">The performance measure P</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-experience-E"><span class="nav-number">3.3.</span> <span class="nav-text">The experience E</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generalization"><span class="nav-number">3.4.</span> <span class="nav-text">Generalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Occam’s-razor"><span class="nav-number">3.5.</span> <span class="nav-text">Occam’s razor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vapnik-Chervonenkis-dimension"><span class="nav-number">3.6.</span> <span class="nav-text">Vapnik-Chervonenkis dimension</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularization"><span class="nav-number">3.7.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Validation-set-and-hyperparameters"><span class="nav-number">3.8.</span> <span class="nav-text">Validation set and hyperparameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Statistics-theory-basic"><span class="nav-number">3.9.</span> <span class="nav-text">Statistics theory basic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Maximum-likelihood-estimation"><span class="nav-number">3.10.</span> <span class="nav-text">Maximum likelihood estimation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stochastic-gradient-descent"><span class="nav-number">3.11.</span> <span class="nav-text">Stochastic gradient descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Challenges-motivating-deep-learning"><span class="nav-number">3.12.</span> <span class="nav-text">Challenges motivating deep learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Local-constancy-and-smoothness-regularization"><span class="nav-number">3.13.</span> <span class="nav-text">Local constancy and smoothness regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Manifold-learning"><span class="nav-number">3.14.</span> <span class="nav-text">Manifold learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Feedforward-Networks"><span class="nav-number">4.</span> <span class="nav-text">Deep Feedforward Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computational-graph-and-backpropagation"><span class="nav-number">4.1.</span> <span class="nav-text">Computational graph and backpropagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learn-conditional-statistics"><span class="nav-number">4.2.</span> <span class="nav-text">Learn conditional statistics</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid-units-for-Bernoulli-Output-distribution"><span class="nav-number">4.3.</span> <span class="nav-text">Sigmoid units for Bernoulli Output distribution:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax-units-for-multinoulli-output-distributions"><span class="nav-number">4.4.</span> <span class="nav-text">Softmax units for multinoulli output distributions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization-for-deep-learning"><span class="nav-number">5.</span> <span class="nav-text">Regularization for deep learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-normalization"><span class="nav-number">5.1.</span> <span class="nav-text">L2 normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping"><span class="nav-number">5.2.</span> <span class="nav-text">Early stopping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parameter-sharing"><span class="nav-number">5.3.</span> <span class="nav-text">parameter sharing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout"><span class="nav-number">5.4.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sparse-Representations"><span class="nav-number">5.5.</span> <span class="nav-text">Sparse Representations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bagging-and-other-ensemble-methods"><span class="nav-number">5.6.</span> <span class="nav-text">Bagging and other ensemble methods</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adversarial-Training"><span class="nav-number">5.7.</span> <span class="nav-text">Adversarial Training</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-learning-training-tips"><span class="nav-number">6.</span> <span class="nav-text">Deep learning training tips</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#testing-the-result-on-training-data-set"><span class="nav-number">6.1.</span> <span class="nav-text">testing the result on training data set</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vanishing-gradient-problem"><span class="nav-number">6.2.</span> <span class="nav-text">Vanishing gradient problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaptive-learning-rate"><span class="nav-number">6.3.</span> <span class="nav-text">Adaptive learning rate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hard-to-find-optimal-network-parameters"><span class="nav-number">6.4.</span> <span class="nav-text">Hard to find optimal network parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum"><span class="nav-number">6.5.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">6.6.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularization-1"><span class="nav-number">6.7.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping-1"><span class="nav-number">6.8.</span> <span class="nav-text">Early stopping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout-1"><span class="nav-number">6.9.</span> <span class="nav-text">Dropout</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kehui Yao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
