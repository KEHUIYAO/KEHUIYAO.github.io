<!doctype html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="study,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="线性代数的几何解释本文素材来自于up主3blue1brown的线性代数视频教程，从几何的角度解释了线性代数，比较有意思。 向量的本质scalar之所以叫scalar是因为它可以scale向量？ 线性组合，张成的空间和基矩阵和线性变换 把矩阵在几何上就是一种对空间的线性变换。 A 2x2 matrix, can be thought as a linear transformation on 2d">
<meta name="keywords" content="study">
<meta property="og:type" content="article">
<meta property="og:title" content="线性代数">
<meta property="og:url" content="http://yoursite.com/2020/09/12/线性代数/index.html">
<meta property="og:site_name" content="Kehui&#39;s Blog">
<meta property="og:description" content="线性代数的几何解释本文素材来自于up主3blue1brown的线性代数视频教程，从几何的角度解释了线性代数，比较有意思。 向量的本质scalar之所以叫scalar是因为它可以scale向量？ 线性组合，张成的空间和基矩阵和线性变换 把矩阵在几何上就是一种对空间的线性变换。 A 2x2 matrix, can be thought as a linear transformation on 2d">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://i.imgur.com/F4awChz.png">
<meta property="og:image" content="https://i.imgur.com/NdJ4cMv.png">
<meta property="og:image" content="https://i.imgur.com/4YSxE2Q.png">
<meta property="og:image" content="https://i.imgur.com/IwurckY.png">
<meta property="og:image" content="https://i.imgur.com/ymdReBD.png">
<meta property="og:image" content="https://i.imgur.com/mVzaqdN.png">
<meta property="og:image" content="https://i.imgur.com/hI9uX9T.png">
<meta property="og:image" content="https://i.imgur.com/eKfb5WL.png">
<meta property="og:image" content="https://i.imgur.com/nViGL4l.png">
<meta property="og:image" content="https://i.imgur.com/eXgq1p8.png">
<meta property="og:image" content="https://i.imgur.com/c9TLjge.png">
<meta property="og:image" content="https://i.imgur.com/7PIzFeU.png">
<meta property="og:updated_time" content="2020-09-22T22:35:49.153Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性代数">
<meta name="twitter:description" content="线性代数的几何解释本文素材来自于up主3blue1brown的线性代数视频教程，从几何的角度解释了线性代数，比较有意思。 向量的本质scalar之所以叫scalar是因为它可以scale向量？ 线性组合，张成的空间和基矩阵和线性变换 把矩阵在几何上就是一种对空间的线性变换。 A 2x2 matrix, can be thought as a linear transformation on 2d">
<meta name="twitter:image" content="https://i.imgur.com/F4awChz.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/09/12/线性代数/">





  <title> 线性代数 | Kehui's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kehui's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For Kevin Durant</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/12/线性代数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kehui Yao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kehui's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                线性代数
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-12T00:00:00-05:00">
                2020-09-12
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-09-22T17:35:49-05:00">
                2020-09-22
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="线性代数的几何解释"><a href="#线性代数的几何解释" class="headerlink" title="线性代数的几何解释"></a>线性代数的几何解释</h3><p>本文素材来自于up主3blue1brown的线性代数视频教程，从几何的角度解释了线性代数，比较有意思。</p>
<h4 id="向量的本质"><a href="#向量的本质" class="headerlink" title="向量的本质"></a>向量的本质</h4><p>scalar之所以叫scalar是因为它可以scale向量？</p>
<h4 id="线性组合，张成的空间和基"><a href="#线性组合，张成的空间和基" class="headerlink" title="线性组合，张成的空间和基"></a>线性组合，张成的空间和基</h4><h4 id="矩阵和线性变换"><a href="#矩阵和线性变换" class="headerlink" title="矩阵和线性变换"></a>矩阵和线性变换</h4><ul>
<li>把矩阵在几何上就是一种对空间的线性变换。</li>
<li>A 2x2 matrix, can be thought as a linear transformation on 2d space, each column represents the  coordinate of the transformed basis vector.</li>
<li>For example, matrix [0,-1;1,0] is 90 degree rotation counterclockwise transformation, since it makes the original x basis vector (1,0) to be (0,1) and original y basis vector (0,1) to be (-1,0).</li>
<li>If two column vectors in a 2x2 matrix are linearly dependent, it means the linear transformation squishes all of 2-D space onto the line where those two vectors sit.</li>
</ul>
<a id="more"></a>
<h4 id="矩阵乘法和线性变换复合"><a href="#矩阵乘法和线性变换复合" class="headerlink" title="矩阵乘法和线性变换复合"></a>矩阵乘法和线性变换复合</h4><p>Consider matrix multiplications as applying linear transformation sequentially.</p>
<h4 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h4><ul>
<li>Some linear transformation stretches space, some squish it. The determinant of a matrix is the special scaling factor by which a linear transformation changes any area.</li>
<li>If a determinant of a transformation is zero, it squishes all of space on to a line, or even onto a single point. So computing whether the determinant of a matrix is zero will give a way of whether  or not the transformation associated with that matrix squishes everything onto a lower dimension.</li>
<li>If the determinant is negative, it means the orientation of space is inverted.</li>
</ul>
<h4 id="逆矩阵，列空间，零空间"><a href="#逆矩阵，列空间，零空间" class="headerlink" title="逆矩阵，列空间，零空间"></a>逆矩阵，列空间，零空间</h4><ul>
<li>Inverse of a matrix is a transformation that has the property: you first apply transformation A, then followed by $A^{-1}$, you will end up back in where you started.</li>
<li>Consider the linear equation system, Ax=b. If A can be inverted, we can use $A^{-1}b$ to solve x. However, if |A| = 0, for example in 2d space, it squishes every input vector onto a line. If b accidentally falls onto this line, there still exist solution x. If not, there is no solution x.</li>
<li>For a 3x3 matrix, its determinant equaling to 0 can either mean it squishes everything onto a plane or squishes everything on to a line. For the latter case, it’s harder for the solution to exist.</li>
<li>Rank of a matrix is defined as the output dimension of the linear transformation associated with the matrix.</li>
<li>If a matrix is not of full rank, say 2x2 matrix with rank 1, the transformation </li>
</ul>
<h4 id="点积和对偶性"><a href="#点积和对偶性" class="headerlink" title="点积和对偶性"></a>点积和对偶性</h4><ul>
<li><p>点积的几何解释：projection one vector onto another, order doesn’t matter, show by symmetricity</p>
</li>
<li><p>Intuition behind dot product: why multiplying pairs and adding them together have anything to do with projection?</p>
<ul>
<li><p>假设空间里有一个向量u</p>
<p><img src="https://i.imgur.com/F4awChz.png" alt="image-20200916203303459"></p>
<p>假设u是一个长度为1的向量，坐标是(ux,uy)，把x轴上的单位向量和u的点积数值是(1,0)*(ux,uy) = ux，而根据对称性可知（在x轴和u之间做一条角平分线），把x轴的单位向量投影到u向量所在的数轴，得到投影的长度也是ux，两者获得了统一。这说明点积的运算法则就是把基向量投影到u上的长度乘u的长度（这里是u的长度是1所以省去。）</p>
</li>
<li><p>左乘一个1x2的矩阵[ux,uy]可以看做是一个2维到1维（数轴）的线性变换（投影变换）,这个变换把原来的基向量映射到了和u在一条直线上的数轴上的两个数，这两个数分别就是[ux,uy]。</p>
</li>
<li><p>This is why doing dot product is projecting a vector onto the span of that unit vector and taking the length</p>
</li>
<li><p>If the u vector is not a unit vector, it’s same thing.</p>
</li>
</ul>
</li>
</ul>
<h4 id="以线性变换的眼光看叉积"><a href="#以线性变换的眼光看叉积" class="headerlink" title="以线性变换的眼光看叉积"></a>以线性变换的眼光看叉积</h4><ul>
<li><p>2d cases, [a,b] x [c,d], calculate the determinant of [a,b;c,d]</p>
</li>
<li><p>3d case[a,b,c] x [c,d,e], <img src="https://i.imgur.com/NdJ4cMv.png" alt="image-20200916215040605"></p>
<p>Since this function goes from three dimensions to one dimension, there will be a 1x3 matrix that encodes this transformation.</p>
<p><img src="https://i.imgur.com/4YSxE2Q.png" alt="image-20200916215223755"></p>
</li>
<li><p>Computationally:</p>
<p><img src="https://i.imgur.com/IwurckY.png" alt="image-20200916215426284"></p>
</li>
<li><p>Geometrical understanding of the formula:</p>
<ul>
<li>Consider the dot product as the length of  projection of (x,y,z) onto p times the length of p.</li>
<li>The right hand side depicts the volume of a 六面体。</li>
<li>六面体的体积公式是底面积乘垂直于底面的高，也就是(x,y,z)向着垂直于由v,w组成平面的直线的投影距离。</li>
<li>So the vector p should have the property that: <ul>
<li>it is perpendicular to the plane formed by v and w. </li>
<li>Its length is equal to the size of the 平行四边形底面</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h4><ul>
<li><p>Consider y = Ax.</p>
</li>
<li><p>Consider y sits in a space where it has its own basis, x sits in a space where it has its own basis.</p>
</li>
<li>matrix A means that 在y的坐标世界里的基向量在x的坐标世界里的坐标是什么</li>
<li>Inverse transformation</li>
<li>我们一个矩阵M可以表示一个线性变换，比如[1,2;1,-1]就表示把原来的基向量(1,0)变成(1,2)，把(0,1)变成(1,-1)。这些都是在x的坐标系里描述的，那我们如何在y的坐标系里描述相同的事情呢。首先我们取一个在y的坐标系下值为[a,b]的vector，通过左乘一个A转化成x坐标系的坐标，然后再左乘M，得到线性变换后在x坐标系下的vector，再左乘一个逆矩阵$A^{-1}$，得到最终[a,b]经过线性变换后在y坐标系里的coordinate值。To summary, it is $A^{-1}MA$</li>
</ul>
<h4 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h4><ul>
<li>经过某个线性变换以后，空间里的某些向量仅进行了拉伸操作，没有被旋转，这些向量就是这个，特征值是负的就表示反向。</li>
<li>rotation的特征向量就是旋转的轴，特征值为1</li>
<li>Av = $\lambda$v, (A-$\lambda$I)v = 0, so det(A-$\lambda I$)=0, meaning that the transformation squishes the space into a lower dimension</li>
<li>A transformation sometimes do not have any eigenvectors like rotation transformation</li>
<li>Shear transformation only has one eigenvectors, because all eigenvectors are the same</li>
<li>Eigenbasis:<ul>
<li>Use eigenvectors as the new basis</li>
<li>Every time we meet a diagona matrix, we can consider it as <strong>在eigenbasis 下</strong> linear transformation 所代表的矩阵。</li>
<li>假设我们有一个矩阵A，我们求出他的特征向量，如果这些特征向量能张成和原来空间一样大，他们就可以作为一组eigenbasis。利用上一章change of basis的知识，我们可以把A这个变换放到eigenbasis所在的空间，看看他是什么样子的。厉害的是，在eigenbasis所在的空间里，这个A所对应的变换被保证是一个对角矩阵所对应的变换，因为这些eigenbasis在这个变换下只进行了缩放操作，并没有旋转！</li>
<li>计算一个不是对角的矩阵 $A^{100}$ 的时候，可以先change basis到eigenbasis，算100次对角矩阵的乘法，在change basis回来。</li>
</ul>
</li>
</ul>
<h3 id="Linear-algebra-for-deep-learning"><a href="#Linear-algebra-for-deep-learning" class="headerlink" title="Linear algebra for deep learning"></a>Linear algebra for deep learning</h3><h4 id="Positive-semidefinite"><a href="#Positive-semidefinite" class="headerlink" title="Positive semidefinite"></a>Positive semidefinite</h4><ul>
<li><p>Formal definition:</p>
<p><img src="https://i.imgur.com/ymdReBD.png" alt="image-20200922164640161"></p>
</li>
<li><p>判断条件：</p>
<ul>
<li>The matrix is PSD If and only if 所有顺序主子式$\geq$0</li>
<li>The matrix is PSD If and only if 所有特征值都$\geq$0</li>
</ul>
</li>
</ul>
<h4 id="Eigendecomposition-of-a-matrix"><a href="#Eigendecomposition-of-a-matrix" class="headerlink" title="Eigendecomposition of a matrix"></a>Eigendecomposition of a matrix</h4><ul>
<li>Let A be a square nxn matrix with n linearly independent eigenvectors $q_i$. Then A can be factorized as <img src="https://i.imgur.com/mVzaqdN.png" alt="image-20200922162927726"> </li>
<li>For nxn real symmetric matrix,:<ul>
<li>the eigenvalues are real.</li>
<li>the eigenvectors can be chosen real and orthonormal. Here is the proof [<a href="https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal]" target="_blank" rel="noopener">https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal]</a></li>
<li>Thus a real symmetric matrix A can be decomposed as <img src="https://i.imgur.com/hI9uX9T.png" alt="image-20200922163122296"> ,where Q is an orthogonal matrix whose columns are the eigenvectors of A, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of A.</li>
</ul>
</li>
</ul>
<h4 id="Symmetric-matrix"><a href="#Symmetric-matrix" class="headerlink" title="Symmetric matrix"></a>Symmetric matrix</h4><ul>
<li>Let A be a real symmetric dxd matrix:<ul>
<li><img src="https://i.imgur.com/eKfb5WL.png" alt="image-20200922164102133">, here $\lambda_1$ is the smallest eigenvalue, and $\lambda_d$ is the largest.</li>
</ul>
</li>
</ul>
<h4 id="Principle-component-analysis"><a href="#Principle-component-analysis" class="headerlink" title="Principle component analysis"></a>Principle component analysis</h4><ul>
<li><p>#### </p>
</li>
</ul>
<h4 id="Singular-value-decomposition"><a href="#Singular-value-decomposition" class="headerlink" title="Singular value decomposition"></a>Singular value decomposition</h4><ul>
<li><p>Formal definition:</p>
<ul>
<li><p>M = $U\Sigma V^T$</p>
</li>
<li><p>where M is a mxn matrix, U is an mxm real or complex unitary(orthonormal if M is real) matrix, $\Sigma$ is an mxn rectangular diagonal matrix with non-negative real numbers on the diagonal, and V is an nxn real or complex unitary matrix</p>
</li>
<li><p>The SVD is not unique. It’s always possible to choose the decomposition so that singular values $\Sigma_{ii}$ are in descending order. In this case, $\Sigma$ is uniquely determined by M.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>Intuitive interpretations:<ul>
<li>In terms of geometrically interpretation:  a SVD is a transformation that contains: rotation, coordinate scaling, and reflection.</li>
</ul>
</li>
<li>Relations to eigenvalue decomposition<img src="https://i.imgur.com/nViGL4l.png" alt="image-20200922173049324"></li>
</ul>
<h4 id="克拉默法则"><a href="#克拉默法则" class="headerlink" title="克拉默法则"></a>克拉默法则</h4><p><img src="https://i.imgur.com/eXgq1p8.png" alt="image-20200909114403555"></p>
<p><img src="https://i.imgur.com/c9TLjge.png" alt="image-20200909114431271"></p>
<h4 id="线性方程组有非零解的条件"><a href="#线性方程组有非零解的条件" class="headerlink" title="线性方程组有非零解的条件"></a>线性方程组有非零解的条件</h4><p><img src="https://i.imgur.com/7PIzFeU.png" alt="image-20200909114859978"></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/study/" rel="tag"># study</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/28/design and experiments tips/" rel="next" title="Design and experiments">
                <i class="fa fa-chevron-left"></i> Design and experiments
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/13/短途旅行/" rel="prev" title="短途旅行">
                短途旅行 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpg" alt="Kehui Yao">
          <p class="site-author-name" itemprop="name">Kehui Yao</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">30</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/KEHUIYAO" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/kehui-yao-a5b770165/" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/KehuiY" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性代数的几何解释"><span class="nav-number">1.</span> <span class="nav-text">线性代数的几何解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#向量的本质"><span class="nav-number">1.1.</span> <span class="nav-text">向量的本质</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性组合，张成的空间和基"><span class="nav-number">1.2.</span> <span class="nav-text">线性组合，张成的空间和基</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵和线性变换"><span class="nav-number">1.3.</span> <span class="nav-text">矩阵和线性变换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵乘法和线性变换复合"><span class="nav-number">1.4.</span> <span class="nav-text">矩阵乘法和线性变换复合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#行列式"><span class="nav-number">1.5.</span> <span class="nav-text">行列式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逆矩阵，列空间，零空间"><span class="nav-number">1.6.</span> <span class="nav-text">逆矩阵，列空间，零空间</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#点积和对偶性"><span class="nav-number">1.7.</span> <span class="nav-text">点积和对偶性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#以线性变换的眼光看叉积"><span class="nav-number">1.8.</span> <span class="nav-text">以线性变换的眼光看叉积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基变换"><span class="nav-number">1.9.</span> <span class="nav-text">基变换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征值与特征向量"><span class="nav-number">1.10.</span> <span class="nav-text">特征值与特征向量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-algebra-for-deep-learning"><span class="nav-number">2.</span> <span class="nav-text">Linear algebra for deep learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Positive-semidefinite"><span class="nav-number">2.1.</span> <span class="nav-text">Positive semidefinite</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Eigendecomposition-of-a-matrix"><span class="nav-number">2.2.</span> <span class="nav-text">Eigendecomposition of a matrix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Symmetric-matrix"><span class="nav-number">2.3.</span> <span class="nav-text">Symmetric matrix</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Principle-component-analysis"><span class="nav-number">2.4.</span> <span class="nav-text">Principle component analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Singular-value-decomposition"><span class="nav-number">2.5.</span> <span class="nav-text">Singular value decomposition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#克拉默法则"><span class="nav-number">2.6.</span> <span class="nav-text">克拉默法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性方程组有非零解的条件"><span class="nav-number">2.7.</span> <span class="nav-text">线性方程组有非零解的条件</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kehui Yao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
