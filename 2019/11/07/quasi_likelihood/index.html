<!doctype html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="1. Quasi-likelihood functions1.1 Independent observations1.1.1 Covariance functionsSuppose the components of the response vector Y are independent with mean vector $\mu$ and covariance matrix $\sigma^">
<meta property="og:type" content="article">
<meta property="og:title" content="Kehui&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/2019/11/07/quasi_likelihood/index.html">
<meta property="og:site_name" content="Kehui&#39;s Blog">
<meta property="og:description" content="1. Quasi-likelihood functions1.1 Independent observations1.1.1 Covariance functionsSuppose the components of the response vector Y are independent with mean vector $\mu$ and covariance matrix $\sigma^">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/Users/kehuiyao/Library/Application%20Support/typora-user-images/image-20191107003451040.png">
<meta property="og:updated_time" content="2020-02-26T22:35:45.701Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kehui&#39;s Blog">
<meta name="twitter:description" content="1. Quasi-likelihood functions1.1 Independent observations1.1.1 Covariance functionsSuppose the components of the response vector Y are independent with mean vector $\mu$ and covariance matrix $\sigma^">
<meta name="twitter:image" content="http://yoursite.com/Users/kehuiyao/Library/Application%20Support/typora-user-images/image-20191107003451040.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/11/07/quasi_likelihood/">





  <title>  | Kehui's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kehui's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For Kevin Durant</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/07/quasi_likelihood/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kehui Yao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kehui's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-07T00:32:25-06:00">
                2019-11-07
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-02-26T16:35:45-06:00">
                2020-02-26
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-Quasi-likelihood-functions"><a href="#1-Quasi-likelihood-functions" class="headerlink" title="1. Quasi-likelihood functions"></a>1. Quasi-likelihood functions</h1><h2 id="1-1-Independent-observations"><a href="#1-1-Independent-observations" class="headerlink" title="1.1 Independent observations"></a>1.1 Independent observations</h2><h3 id="1-1-1-Covariance-functions"><a href="#1-1-1-Covariance-functions" class="headerlink" title="1.1.1 Covariance functions"></a>1.1.1 Covariance functions</h3><p>Suppose the components of the response vector Y are independent with mean vector $\mu$ and covariance matrix $\sigma^2V(\mu)$, where $\sigma^2$ may be unknown but $V(\mu)$ is a matrix of known functions. It’s assumed that the parameters of interest $\beta$ is related to the dependence of $\mu$ on covariates $x$. Under independent assumption, $\mathbf{V}(\boldsymbol{\mu})=\operatorname{diag}\left{V_{1}(\boldsymbol{\mu}), \ldots, V_{n}(\boldsymbol{\mu})\right}$, and in the majority of applications the functions $V_1,\ldots,V_n$ may be taken to be identical. </p>
<h3 id="1-1-2-Construction-of-the-quasi-likelihood-function"><a href="#1-1-2-Construction-of-the-quasi-likelihood-function" class="headerlink" title="1.1.2 Construction of the quasi-likelihood function"></a>1.1.2 Construction of the quasi-likelihood function</h3><h4 id="1-1-2-1-Preliminary-knowledge-Fisher-information"><a href="#1-1-2-1-Preliminary-knowledge-Fisher-information" class="headerlink" title="1.1.2.1 Preliminary knowledge: Fisher information"></a>1.1.2.1 Preliminary knowledge: Fisher information</h4><p>The <a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener">partial derivative</a> with respect to <em>θ</em> of the <a href="https://en.wikipedia.org/wiki/Natural_logarithm" target="_blank" rel="noopener">natural logarithm</a> of the likelihood function is called the “<em><a href="https://en.wikipedia.org/wiki/Score_(statistics" target="_blank" rel="noopener">score</a>)</em>”. Under certain regularity conditions, if <em>θ</em> is the true parameter (i.e. <em>X</em> is actually distributed as <em>f</em>(<em>X</em>; <em>θ</em>)), it can be shown that the <a href="https://en.wikipedia.org/wiki/Expected_value" target="_blank" rel="noopener">expected value</a> (the first <a href="https://en.wikipedia.org/wiki/Moment_(mathematics" target="_blank" rel="noopener">moment</a>)) of the score is 0.</p>
<p>Proof:<br>$$<br>\begin{aligned} \mathrm{E}\left[\frac{\partial}{\partial \theta} \log f(X ; \theta) | \theta\right] &amp;=\int \frac{\frac{\partial}{\partial \theta} f(x ; \theta)}{f(x ; \theta)} f(x ; \theta) d x \ &amp;=\frac{\partial}{\partial \theta} \int f(x ; \theta) d x \ &amp;=\frac{\partial}{\partial \theta} 1=0 \end{aligned}<br>$$<br>The <a href="https://en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener">variance</a> of the score is defined to be the <strong>Fisher information</strong>:<br>$$<br>\mathcal{I}(\theta)=\mathrm{E}\left[\left(\frac{\partial}{\partial \theta} \log f(X ; \theta)\right)^{2} | \theta\right]=\int\left(\frac{\partial}{\partial \theta} \log f(x ; \theta)\right)^{2} f(x ; \theta) d x<br>$$<br>The Fisher information may also be written as:<br>$$<br>\mathcal{I}(\theta)=-\mathrm{E}\left[\frac{\partial^{2}}{\partial \theta^{2}} \log f(X ; \theta) | \theta\right]<br>$$<br>Since<br>$$<br>\frac{\partial^{2}}{\partial \theta^{2}} \log f(X ; \theta)=\frac{\frac{\partial^{2}}{\partial \theta^{2}} f(X ; \theta)}{f(X ; \theta)}-\left(\frac{\frac{\partial}{\partial \theta} f(X ; \theta)}{f(X ; \theta)}\right)^{2}=\frac{\frac{\partial^{2}}{\partial \theta^{2}} f(X ; \theta)}{f(X ; \theta)}-\left(\frac{\partial}{\partial \theta} \log f(X ; \theta)\right)^{2}<br>$$<br>and<br>$$<br>\mathrm{E}\left[\frac{\frac{\partial^{2}}{\partial \theta^{2}} f(X ; \theta)}{f(X ; \theta)} | \theta\right]=\frac{\partial^{2}}{\partial \theta^{2}} \int f(x ; \theta) d x=0<br>$$</p>
<h4 id="1-1-2-2-Cramer-Rao-bound"><a href="#1-1-2-2-Cramer-Rao-bound" class="headerlink" title="1.1.2.2 Cramer-Rao bound"></a>1.1.2.2 Cramer-Rao bound</h4><h5 id="Scalar-case"><a href="#Scalar-case" class="headerlink" title="Scalar case"></a>Scalar case</h5><p>In <a href="https://en.wikipedia.org/wiki/Estimation_theory" target="_blank" rel="noopener">estimation theory</a> and <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank" rel="noopener">statistics</a>, the <strong>Cramér–Rao bound (CRB)</strong>, <strong>Cramér–Rao lower bound (CRLB)</strong>, <strong>Cramér–Rao inequality</strong>, <strong>Fréchet–Darmois–Cramér–Rao inequality</strong>, or <strong>information inequality</strong> expresses a lower bound on the <a href="https://en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener">variance</a> of unbiased <a href="https://en.wikipedia.org/wiki/Estimator" target="_blank" rel="noopener">estimators</a> of a deterministic (fixed, though unknown) parameter. In its simplest form, the bound states that the variance of any <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator" target="_blank" rel="noopener">unbiased</a> estimator is at least as high as the inverse of the <a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank" rel="noopener">Fisher information</a></p>
<p>Suppose $\theta$ is the parameter, $x$ is the measurement. The fisher information is $I(\theta)$, and we have:<br>$$<br>\operatorname{var}(\hat{\theta}) \geq \frac{1}{I(\theta)}<br>$$<br>where<br>$$<br>I(\theta)=n \times \mathrm{E}\left[\left(\frac{\partial \ell(x ; \theta)}{\partial \theta}\right)^{2}\right]=n \times-\mathrm{E}\left[\frac{\partial^{2} \ell(x ; \theta)}{\partial \theta^{2}}\right]<br>$$</p>
<h5 id="Multivariate-case"><a href="#Multivariate-case" class="headerlink" title="Multivariate case"></a>Multivariate case</h5><p>Extending the Cramér–Rao bound to multiple parameters, define a parameter column vector with<br>$$<br>\boldsymbol{\theta}=\left[\theta_{1}, \theta_{2}, \ldots, \theta_{d}\right]^{T} \in \mathbb{R}^{d}<br>$$<br>The Fisher information matrix is a $d\times d$ matrix with element $I_{m,k}$ defined as<br>$$<br>I_{m, k}=\mathrm{E}\left[\frac{\partial}{\partial \theta_{m}} \log f(x ; \boldsymbol{\theta}) \frac{\partial}{\partial \theta_{k}} \log f(x ; \boldsymbol{\theta})\right]=-\mathrm{E}\left[\frac{\partial^{2}}{\partial \theta_{m} \partial \theta_{k}} \log f(x ; \boldsymbol{\theta})\right]<br>$$<br>Let $\boldsymbol{T}(X)$ be an estimator for any vector function of parameters, and<br>$$<br>\boldsymbol{T}(X)=\left(T_{1}(X), \ldots, T_{d}(X)\right)^{T}<br>$$<br>and denote its expectation vector by $\psi(\boldsymbol{\theta})$. The Cramer-Rao bound then states that the covariance matrix of $\boldsymbol{T}(X)$ satisfies<br>$$<br>\operatorname{cov}_{\boldsymbol{\theta}}(\boldsymbol{T}(X)) \geq \frac{\partial \boldsymbol{\psi}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}[I(\boldsymbol{\theta})]^{-1}\left(\frac{\partial \boldsymbol{\psi}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\right)^{T}<br>$$<br>Where:</p>
<ol>
<li><p>The matrix inequality $A \geq B$ is understood to mean that the matrix $A - B$ is positive semidefinite.</p>
</li>
<li><p>$\partial \psi(\boldsymbol{\theta}) / \partial \boldsymbol{\theta}$ is the Jacobian matrix whose $ij$ element is given by $\partial \psi_{i}(\boldsymbol{\theta}) / \partial \theta_{j}$.</p>
</li>
</ol>
<p>If $\boldsymbol{\psi}(\boldsymbol{\theta})=\boldsymbol{\theta}$, then the Cramer-Rao bound reduces to<br>$$<br>\operatorname{cov}_{\boldsymbol{\theta}}(\boldsymbol{T}(X)) \geq I(\boldsymbol{\theta})^{-1}<br>$$</p>
<h5 id="Single-parameter-proof"><a href="#Single-parameter-proof" class="headerlink" title="Single-parameter proof"></a>Single-parameter proof</h5><p>Let $T = t(X)$ is a estimator satisfying $\mathrm{E}(T)=\psi(\theta)$. The goal is to prove that, for all $\theta$,<br>$$<br>\operatorname{var}(t(X)) \geq \frac{\left[\psi^{\prime}(\theta)\right]^{2}}{I(\theta)}<br>$$<br>Define V as the score of $\theta$ , $E(V) = 0$,<br>$$<br>\begin{aligned} \operatorname{cov}(V, T) &amp;=\mathrm{E}\left(T \cdot\left[\frac{1}{f(X ; \theta)} \frac{\partial}{\partial \theta} f(X ; \theta)\right]\right) \ &amp;=\int t(x)\left[\frac{1}{f(x ; \theta)} \frac{\partial}{\partial \theta} f(x ; \theta)\right] f(x ; \theta) d x \ &amp;=\frac{\partial}{\partial \theta}\left[\int t(x) f(x ; \theta) d x\right]=\psi^{\prime}(\theta) \end{aligned}<br>$$<br>The <a href="https://en.wikipedia.org/wiki/Cauchy–Schwarz_inequality" target="_blank" rel="noopener">Cauchy–Schwarz inequality</a> shows that:<br>$$<br>\sqrt{\operatorname{var}(T) \operatorname{var}(V)} \geq|\operatorname{cov}(V, T)|=\left|\psi^{\prime}(\theta)\right|<br>$$<br>Therefore,<br>$$<br>\operatorname{var}(T) \geq \frac{\left[\psi^{\prime}(\theta)\right]^{2}}{\operatorname{var}(V)}=\frac{\left[\psi^{\prime}(\theta)\right]^{2}}{I(\theta)}<br>$$<br>which completes the proof.</p>
<h4 id="1-1-2-3-First-single-component-of-vector-Y"><a href="#1-1-2-3-First-single-component-of-vector-Y" class="headerlink" title="1.1.2.3 First single component of vector Y"></a>1.1.2.3 First single component of vector Y</h4><p>Suppose we have only one observation, and write function $<br>U=u(\mu ; Y)=\frac{Y-\mu}{\sigma^{2} V(\mu)}$. This function $U$ has the following properties in common with a log-likelihood derivative because<br>$$<br>\begin{aligned} E(U) &amp;=0 \ \operatorname{var}(U) &amp;=1 /\left{\sigma^{2} V(\mu)\right} \-E\left(\frac{\partial U}{\partial \mu}\right) &amp;=1 /\left{\sigma^{2} V(\mu)\right} \end{aligned}<br>$$<br>since $E(Y) = \mu$, and<br>$$<br>\begin{aligned}-E\left(\frac{\partial U}{\partial \mu}\right) &amp;=-E\left(-\frac{1}{\sigma^{2} V(\mu)}-\frac{(Y-\mu) V^{\prime}(\mu)}{\sigma^{2} V^{2}(\mu)}\right) \ &amp;=\frac{1}{\sigma^{2} V(\mu)} \end{aligned}<br>$$<br>So function $U$ just behave like the first order derivative of the loglikelihood function of $\mu$, so this gives us the idea to define the quasi-likelihood for a single data point as<br>$$<br>Q(\mu ; y)=\int_{y}^{\mu} \frac{y-t}{\sigma^{2} V(t)} d t<br>$$<br>Since the take the first order derivative of $\mu$ on $Q(\mu ; y)$ gives us $U$.</p>
<h4 id="1-1-2-4-Sum-of-the-individual-contributions"><a href="#1-1-2-4-Sum-of-the-individual-contributions" class="headerlink" title="1.1.2.4 Sum of the individual contributions"></a>1.1.2.4 Sum of the individual contributions</h4><p>Since the components of Y are independent by assumption, the quasi-likelihood for the complete data is the sum of the individual contributions:<br>$$<br>Q(\boldsymbol{\mu} ; \mathbf{y})=\sum Q_{i}\left(\mu_{i} ; y_{i}\right)<br>$$<br><img src="/Users/kehuiyao/Library/Application Support/typora-user-images/image-20191107003451040.png" alt="image-20191107003451040"></p>
<p>The above shows a table of quasi-likelihood associated with some simple variance functions.</p>
<h3 id="1-1-3-Quasi-deviance-function"><a href="#1-1-3-Quasi-deviance-function" class="headerlink" title="1.1.3 Quasi-deviance function"></a>1.1.3 Quasi-deviance function</h3><p>By analogy,the quasi-deviance function corresponding to a single observation is<br>$$<br>D(y ; \mu)=-2 \sigma^{2} Q(\mu ; y)=2 \int_{\mu}^{y} \frac{y-t}{V(t)} d t<br>$$<br>The total deviance can be obtainedby adding over the components, and is a computable function depending on $y$ and $\mu$ alone, it does not depend on $\sigma^2$.</p>
<h3 id="1-1-4-Parameter-estimation"><a href="#1-1-4-Parameter-estimation" class="headerlink" title="1.1.4 Parameter estimation"></a>1.1.4 Parameter estimation</h3><h4 id="1-1-4-1-Newton-Raphson-iteration-of-estimating-beta"><a href="#1-1-4-1-Newton-Raphson-iteration-of-estimating-beta" class="headerlink" title="1.1.4.1 Newton-Raphson iteration of estimating $\beta$"></a>1.1.4.1 Newton-Raphson iteration of estimating $\beta$</h4><p>The quasi-likelihood estimating equations for the regression parameters $\beta$, obtained by differentiating $Q(\mu;y)$, may be written in the form $U(\hat{\beta})=0$, where<br>$$<br>\mathbf{U}(\boldsymbol{\beta})=\mathbf{D}^{T} \mathbf{V}^{-1}(\mathbf{Y}-\boldsymbol{\mu}) / \sigma^{2}<br>$$<br>The components of D, of order $n \times p$ , satisfy $D_{i r}=\partial \theta_{i} / \partial \beta_{r}$. And V is the covariance matrix of Y. The above formula can be verified by example, say n = 2, and k = 2. We have<br>$$<br>T=\int_{y_{1}}^{x_{11} b_{1}+x_{12} b_{2}} \frac{t-y_{1}}{\sigma^{2} V\left(u_{1}\right)} d t+\int_{y_{2}}^{x_{21} b_{1}+x_{22} b 2} \frac{t-y_{2}}{\sigma^{2} V\left(u_{2}\right)} d t<br>$$</p>
<p>$$<br>\left(\begin{array}{l}{\frac{\partial T}{\partial b_{1}}} \ {\frac{\partial T}{\partial b_{2}}}\end{array}\right)=U(\beta)=\mathbf{D}^{T} \mathbf{V}^{-1}(\mathbf{Y}-\boldsymbol{\mu}) / \sigma^{2}<br>$$</p>
<p>Since:<br>$$<br>\frac{\partial \int_{a}^{f(b)} x d x}{\partial b}=f^{\prime}(b) \cdot f(b)<br>$$<br>The convariacne matrix of $\mathbf{U}(\boldsymbol{\beta})$, which can be derived using the fisher information and the Cramer-rao lower bound which is discussed in the preliminary knowledge. so:</p>
<p>How can I get this $\mathbf{i}<em>{\beta}$?<br>$$<br>\mathbf{i}</em>{\beta}=\mathbf{D}^{T} \mathbf{V}^{-1} \mathbf{D} / \sigma^{2}<br>$$</p>
<p>$$<br>\operatorname{cov}(\hat{\boldsymbol{\beta}}) \simeq \mathbf{i}_{\beta}^{-1}=\sigma^{2}\left(\mathbf{D}^{T} \mathbf{V}^{-1} \mathbf{D}\right)^{-1}<br>$$</p>
<p>Beginning with an arbitrary value $\hat{\beta_0}$ sufficiently close to $\hat{\beta}$, the sequence of parameter estimates generated by the Newton-Raphson method with Fisher scoring is<br>$$<br>\hat{\boldsymbol{\beta}}<em>{1}=\hat{\boldsymbol{\beta}}</em>{0}+\left(\hat{\mathbf{D}}<em>{0}^{T} \hat{\mathbf{V}}</em>{0}^{-1} \hat{\mathbf{D}}<em>{0}\right)^{-1} \hat{\mathbf{D}}</em>{0}^{T} \hat{\mathbf{V}}<em>{0}^{-1}\left(\mathbf{y}-\hat{\boldsymbol{\mu}}</em>{0}\right)<br>$$</p>
<h4 id="1-1-4-2-Details-of-newton-raphson-iteration-used-on-the-method-of-scoring"><a href="#1-1-4-2-Details-of-newton-raphson-iteration-used-on-the-method-of-scoring" class="headerlink" title="1.1.4.2 Details of newton-raphson iteration used on the method of scoring"></a>1.1.4.2 Details of newton-raphson iteration used on the method of scoring</h4><p>Let $l(\theta)$ be the likelihood function, and $S(\theta)$ be the score function, and<br>$$<br>J(\theta)=J(\theta ; X)=-\frac{\partial}{\partial \theta} S(\theta)=-\frac{\partial^{2}}{\partial \theta^{2}} l(\theta)<br>$$<br>Then, we take an initial value $\theta_0$ and write<br>$$<br>\theta_{1}=\theta_{0}+J\left(\theta_{0}\right)^{-1} S\left(\theta_{0}\right)<br>$$<br>and we keep repeating this procedure as long as $\left|S\left(\theta_{j}\right)\right|&gt;\epsilon$.<br>$$<br>\theta_{k+1}=\theta_{k}+J\left(\theta_{k}\right)^{-1} S\left(\theta_{0}\right)<br>$$<br>If $\hat{\theta}$ is a local maximum for the likelihood function, we must have<br>$$<br>J(\hat{\theta})=-\frac{\partial^{2}}{\partial \theta^{2}} l(\hat{\theta})&gt;0<br>$$<br>And the quantity $J(\hat{\theta})$ determines the sharpness of the peak in the likelihood fuction around the maximum. It is also known as the observed information.</p>
<p>So in the section above, $(\hat{\mathbf{D}}<em>{0}^{T} \hat{\mathbf{V}}</em>{0}^{-1} \hat{\mathbf{D}}<em>{0})^{-1}$ can be thought as $J\left(\theta</em>{k}\right)^{-1}$, and   $\hat{\mathbf{D}}<em>{0}^{T} \hat{\mathbf{V}}</em>{0}^{-1}\left(\mathbf{y}-\hat{\boldsymbol{\mu}}_{0}\right)$ can be thought as $S(\theta_0)$.</p>
<h4 id="1-1-4-3-Estimation-of-sigma-2"><a href="#1-1-4-3-Estimation-of-sigma-2" class="headerlink" title="1.1.4.3 Estimation of $\sigma^{2}$"></a>1.1.4.3 Estimation of $\sigma^{2}$</h4><p>$$<br>\tilde{\sigma}^{2}=\frac{1}{n-p} \sum_{i}\left(Y_{i}-\hat{\mu}<em>{i}\right)^{2} / V</em>{i}\left(\hat{\mu}_{i}\right)=X^{2} /(n-p)<br>$$</p>
<h2 id="1-2-Dependent-observations"><a href="#1-2-Dependent-observations" class="headerlink" title="1.2 Dependent observations"></a>1.2 Dependent observations</h2><h3 id="1-2-1-Quasi-likelihood-estimating-equations"><a href="#1-2-1-Quasi-likelihood-estimating-equations" class="headerlink" title="1.2.1 Quasi-likelihood estimating equations"></a>1.2.1 Quasi-likelihood estimating equations</h3><p>Suppose now that $\operatorname{cov}(\mathbf{Y})=\sigma^{2} \mathbf{V}(\boldsymbol{\mu})$ where $\mathbf{V}(\boldsymbol{\mu})$ is a symmetric positive-definite $n \times n$ matrix of known functions $V_{i j}(\mu)$, no longer diagonal. The score function with components $U_{r}(\boldsymbol{\beta})$, has the following properties:<br>$$<br>E\left{U_{r}(\boldsymbol{\beta})\right}=0<br>$$</p>
<p>$$<br>\begin{aligned} \operatorname{cov}{\mathbf{U}(\boldsymbol{\beta})} &amp;=\mathbf{D}^{T} \mathbf{V}^{-1} \mathbf{D} / \sigma^{2}=\mathbf{i}<em>{\beta} \-E\left(\frac{\partial U</em>{\boldsymbol{r}}(\boldsymbol{\beta})}{\partial \beta_{\boldsymbol{s}}}\right) &amp;=\mathbf{D}^{T} \mathbf{V}^{-1} \mathbf{D} / \sigma^{2} \end{aligned}<br>$$</p>
<p>Similarly:<br>$$<br>\mathbf{U}(\hat{\boldsymbol{\beta}})=\hat{\mathbf{D}}^{T} \hat{\mathbf{V}}^{-1}(\mathbf{Y}-\hat{\boldsymbol{\mu}})=\mathbf{0}<br>$$<br>is approximately unbiased for $\beta$ and asymptotically normally distributed with limiting variance $\operatorname{cov}(\hat{\boldsymbol{\beta}}) \simeq \sigma^{2}\left(\mathbf{D}^{T} \mathbf{V}^{-1} \mathbf{D}\right)^{-1}=\mathbf{i}_{\beta}^{-1}$.  </p>
<h1 id="2-论文基本思路："><a href="#2-论文基本思路：" class="headerlink" title="2. 论文基本思路："></a>2. 论文基本思路：</h1><h4 id="Candidate-model"><a href="#Candidate-model" class="headerlink" title="Candidate model:"></a>Candidate model:</h4><p>论文解决一个聚类问题，聚类问题有两个方面：一个是如何确定聚类个数，另一个是在给定聚类个数后如何分类。本文在解决后一个问题的过程中用了candidate model 的思路，也就是给定很多种聚类方案，通过计算一些统计量，设置一些判断准则的方法，判断哪种聚类方案是最好的。</p>
<h4 id="Model-and-Estimation"><a href="#Model-and-Estimation" class="headerlink" title="Model and Estimation"></a>Model and Estimation</h4><p>$$<br>E\left(Y_{i} | \epsilon_{i}\right)=h\left(\alpha+\boldsymbol{x}<em>{i}^{\prime} \boldsymbol{\beta}+\gamma</em>{\omega} \omega_{i}+\epsilon_{i}\right)<br>$$</p>
<p>$\alpha$ denotes the intercept, $\beta=\left(\beta_{1}, \dots, \beta_{p}\right)^{\prime}$ denotes a vector of p slopes associated with the covariates, and $\gamma_w$ denotes a “jump coefficient “ associated with the cluster $\Omega$. $\epsilon_i$ has variance $\sigma^2$ and sptial correlation $\rho_{i, i^{\prime}} \text { between } s_{i} \text { and } s_{i^{\prime}}$.<br>$$<br>\lambda_{i}=E\left{E\left(Y_{i} | \epsilon_{i}\right)\right}=h\left(\alpha+\boldsymbol{x}<em>{i}^{\prime} \boldsymbol{\beta}+\gamma</em>{\omega} \omega_{i}+\sigma_{\tau}\right)<br>$$<br>Where $\sigma_{\tau}^{2}=(1 / 2) \sigma^{2}$ when the link function h is exponential.</p>
<p>Let $\boldsymbol{V}=\operatorname{var}(\boldsymbol{Y})$ denotes the marginal cov matrix of the response vector. Let candidates model be $\omega=\left(\omega_{1}, \ldots, \omega_{n}\right)^{\prime} $  and complementary model be $\omega^{c}=\left(\omega_{1}^{c}, \ldots, \omega_{n}^{c}\right)^{\prime}$. </p>
<p>We have $\boldsymbol{\lambda}<em>{\omega^{c}}=h\left(\alpha^{c}+\boldsymbol{X} \boldsymbol{\beta}+\gamma</em>{\omega^{c}} \boldsymbol{\omega}^{c}+\sigma_{\tau}\right)$, $\text { where } \alpha^{c}=\alpha+\gamma_{\omega} \text { and } \gamma_{\omega^{c}}=-\gamma_{\omega} . \text { since } \boldsymbol{\lambda}<em>{\omega} \equiv \boldsymbol{\lambda}</em>{\omega^{c}}$</p>
<p>Apply the QLE function to estimate $\beta,\gamma_{\omega}, \alpha$ and $\gamma_{u_{j}^{\circ}}, \alpha^{c}, \text { and } \beta_{u_{j}^{c}}$  in the complement model. </p>
<h4 id="Determine-which-candidate-model-is-the-best"><a href="#Determine-which-candidate-model-is-the-best" class="headerlink" title="Determine which candidate model is the best"></a>Determine which candidate model is the best</h4><p>如果candidate model 足够好，我们可以发现$\left{h\left(\hat{\gamma}_{u_{j}}\right)-h\left(\hat{\gamma}_{u_{j}^{c}}\right)\right}^{2}$ 这一项的值应该是足够大的。因为最好的结果会是$\left{h\left(\gamma_{\omega}\right)-h\left(-\gamma_{\omega}\right)\right}^{2}$. 所以，我们用<br>$$<br>\hat{\omega}=\arg \max <em>{\boldsymbol{u}</em>{j} \in \mathbf{T}}\left{h\left(\hat{\gamma}_{u_{j}}\right)-h\left(\hat{\gamma}_{u_{j}^{c}}\right)\right}^{2}<br>$$<br>来决定哪个candidate model最好。</p>
<h4 id="Three-step-algorithm-to-wrap-it-up"><a href="#Three-step-algorithm-to-wrap-it-up" class="headerlink" title="Three-step algorithm to wrap it up"></a>Three-step algorithm to wrap it up</h4><p>这个算法的目的是通过迭代法，不停调整Y协方差矩阵的估计，最优candiate模型的估计，以及模型参数的估计。具体步骤如下：</p>
<ol>
<li>给定一堆candidate model，给定误差的协方差矩阵，分别用qle对这些model求得参数估计，用上述的criteria求得最好的模型，记录下来。</li>
<li>用这个最好的模型拟合数据，记录残差，反过来重新估计误差的协方差矩阵。</li>
<li>用新得到的协方差矩阵，再看这堆candidate model哪个最好，如果还是上次迭代时得到的那个，那算法终止，所有在当前估计得到的参数会作为最终结果输出。</li>
</ol>
<h4 id="Identification-of-multiple-clusters"><a href="#Identification-of-multiple-clusters" class="headerlink" title="Identification of multiple clusters"></a>Identification of multiple clusters</h4><p>Use quasi-deviance (QDEV) method for model selection. $ D\left(\boldsymbol{\lambda}<em>{\omega</em>{1}, \omega_{2}}, \boldsymbol{\lambda}<em>{\omega</em>{1}}\right)= \left(\boldsymbol{\lambda}<em>{1, \omega</em>{2}}-\boldsymbol{\lambda}_{w_{1}}\right)^{\prime}\left{\boldsymbol{V}_{w_{1}, \omega_{2}}^{-1}\left(\boldsymbol{Y}-\boldsymbol{\lambda}_{w_{1}, \mu_{2}}\right)+\boldsymbol{V}_{w_{1}}^{-1}\left(\boldsymbol{Y}-\boldsymbol{\lambda}_{w_{1}}\right)\right}$ can be used to compare models with different clusters. Lin (2011) showed that $D\left(\boldsymbol{\lambda}<em>{\omega</em>{1}, \omega_{2}}, \boldsymbol{\lambda}<em>{\omega</em>{1}}\right)$ converges in distribution to a chi-squared distribution $\chi_{1}^{2}$ with one degree of freedom.</p>
<p>具体步骤：</p>
<p>To determine whether an additional cluster exists, we update the collection of status vectors to be $\Upsilon_{1}=\left{\boldsymbol{u} \cdot \hat{\boldsymbol{\omega}}<em>{1}^{c}: \boldsymbol{u} \in \Upsilon\right}$, where $\hat{\omega}</em>{1}^{c}=1-\hat{\omega}_{1}$. That is, we exclude the spatial units identified from each status vector in $\Upsilon$.</p>
<ul>
<li><p>Let $\boldsymbol{\lambda}<em>{\partial</em>{1}}=h\left{\left(\alpha+\sigma_{\tau}\right)+\boldsymbol{X} \boldsymbol{\beta}+\gamma_{\hat{\omega}<em>{1}} \hat{\boldsymbol{\omega}}</em>{1}\right}$ denote the single-cluster model associated with the estimated status vector $\hat{\omega}_{1}$. </p>
</li>
<li><p>Let $u_{j}^{<em>}, j=1, \ldots, J-1$, denote a candidate vector in $\Upsilon_{1}$, and let $u_{j}^{</em> c}=\left(1-u_{j}^{<em>}\right) \hat{\omega}<em>{1}^{c}$ denote its complement. We denote the two-cluster model as.$\boldsymbol{\lambda}</em>{\dot{\omega}<em>{1}, u</em>{j}^{</em>}}=h\left{\left(\alpha+\sigma_{\tau}\right)+\boldsymbol{X} \boldsymbol{\beta}+\gamma_{\omega_{1}} \hat{\boldsymbol{\omega}}<em>{1}+\gamma</em>{u_{j}^{<em>}} \boldsymbol{u}_{j}^{</em>}\right}$.The three-step computational algorithm can be applied to ﬁt model $\lambda_{\dot{\alpha}<em>{1}, u</em>{j}^{*}}$ .</p>
</li>
</ul>
<h4 id="Simulation-based-on-a-hierarchical-generalized-linear-model"><a href="#Simulation-based-on-a-hierarchical-generalized-linear-model" class="headerlink" title="Simulation based on a hierarchical generalized linear model"></a>Simulation based on a hierarchical generalized linear model</h4><p>Given $\epsilon_{i}$, the response $Y_i$ was independently Poisson with mean $E\left{Y_{i} ; \epsilon_{i}\right}=\exp \left(\beta_{0}^{*}+\beta_{1} x_{i, 1}+\beta_{2} x_{i, 2}+\beta_{3} x_{i, 3}+\epsilon_{i}\right)$. The model then had:<br>$$<br>\begin{aligned} \theta_{i} &amp;=E\left(Y_{i}\right)=\exp \left{\beta_{0}+\beta_{1} x_{i, 1}+\beta_{2} x_{i, 2}+\beta_{3} x_{i, 3}\right} \ \operatorname{var}\left(Y_{i}\right) &amp;=\theta_{i}+\theta_{i}^{2}{\exp (1)-1} \text { and } \operatorname{cov}\left(Y_{i}, Y_{j}\right)=\theta_{i} \theta_{j}\left{\exp \left(\rho_{i j}\right)-1\right} \end{aligned}<br>$$</p>
<h4 id="具体代码实现中的技巧："><a href="#具体代码实现中的技巧：" class="headerlink" title="具体代码实现中的技巧："></a>具体代码实现中的技巧：</h4><ul>
<li><p>起初协方差矩阵就是一个单位阵，扫过所有可能的candidate model，用qle maximize value function，记录下来，最终选取得到最大value的那个candidate model，用qle得到其参数估计。</p>
</li>
<li><p>把这些估计得到的参数代入计算残差，用残差重新估计协方差矩阵，并在这个重新估计得到的协方差矩阵的基础上，重新扫一遍candidate model，用qle maximize value function，记录下来，选取最大value的那个candidate，并比较这个candidate是否和第一步得到的candidate是同一个，如果是的，算法终止，如果不是，就得重复第一第二步，直至收敛。</p>
</li>
<li><p>再完成第一第二步以后，第三步是看是否要新增一个cluster。方法是用第二步得到的协方差矩阵的估计，在已有的candidate model基础上，再加上一列vector，扫一遍这一列vector的所有可能取值，并取qle能得到的最大value的那个candidate vector，然后用qdev function计算加上这一列vector后，拟合效果是否比以前好很多（有一个判断准则），如果没有好很多，那么就不要加上了，如果好很多，那就得加上，然后重新估计协方差矩阵。理论上还需要根据新的协方差矩阵再次扫一遍所有candidate vector来confirm这个vector（参考第一步和第二步），但代码里略去了这一步。</p>
</li>
</ul>
<h1 id="3-Rcpp-基础知识"><a href="#3-Rcpp-基础知识" class="headerlink" title="3. Rcpp 基础知识"></a>3. Rcpp 基础知识</h1><h2 id="Rcpp-Armadillo-example-code-这是quasi-likelihood-function的c-inplementation"><a href="#Rcpp-Armadillo-example-code-这是quasi-likelihood-function的c-inplementation" class="headerlink" title="Rcpp Armadillo example code: (这是quasi likelihood function的c++ inplementation)"></a>Rcpp Armadillo example code: (这是quasi likelihood function的c++ inplementation)</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;RcppArmadillo.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> arma;</span><br><span class="line"><span class="comment">// [[Rcpp::depends(RcppArmadillo)]]</span></span><br><span class="line"><span class="comment">// [[Rcpp::export]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Rcpp::<span class="function">List <span class="title">qle</span><span class="params">(vec beta_in, mat X_in, vec R_in, <span class="keyword">double</span> n_i, mat Corr, <span class="keyword">double</span> sigma2, vec fix_par)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> p = beta_in.size();</span><br><span class="line">	<span class="keyword">int</span> n = X_in.n_rows;</span><br><span class="line">	vec beta_in0 = vec(beta_in);</span><br><span class="line">	mat X = mat(X_in);</span><br><span class="line"></span><br><span class="line">	mat V1 = <span class="built_in">exp</span>(mat(n,n).fill(sigma2)%Corr)-mat(n,n).fill(<span class="number">1</span>);</span><br><span class="line">	mat invI = mat(p,p);</span><br><span class="line">	invI.zeros();</span><br><span class="line">	mat V = mat(n,n);</span><br><span class="line">	mat D = mat(n,p);</span><br><span class="line">	mat mu = vec(n);</span><br><span class="line">	mat invV = mat(n,n);</span><br><span class="line">	<span class="keyword">int</span> iter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">try</span></span><br><span class="line">	&#123;</span><br><span class="line">	   <span class="keyword">while</span> (iter &lt;= <span class="number">400</span>)&#123;</span><br><span class="line">		iter++;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		mu = vec(n).fill(n_i)%<span class="built_in">exp</span>(X*beta_in+vec(n).fill(<span class="number">0.5</span>*sigma2))%<span class="built_in">exp</span>(vec(n).fill(sum(fix_par)));</span><br><span class="line">		</span><br><span class="line">		D = repmat(mu, <span class="number">1</span>, p)%X; </span><br><span class="line"></span><br><span class="line">		V = V1%(mu*mu.t());</span><br><span class="line">	</span><br><span class="line">		</span><br><span class="line">		V.diag() = mu+vec(n).fill(<span class="built_in">exp</span>(sigma2))%mu%mu-mu%mu;</span><br><span class="line">		invV = inv(V);</span><br><span class="line">		vec beta1 = vec(beta_in);</span><br><span class="line">		invI = inv(D.t()*invV*D);</span><br><span class="line">		beta_in = beta_in+invI*D.t()*invV*(R_in-mu);</span><br><span class="line">		<span class="keyword">if</span> (sum((beta1-beta_in)%(beta1-beta_in)) &lt; <span class="number">0.01</span>)&#123;<span class="keyword">break</span>;&#125;</span><br><span class="line">  		&#125;</span><br><span class="line">	  	<span class="keyword">if</span> (iter == <span class="number">400</span>)&#123;</span><br><span class="line">	  		beta_in = beta_in0;</span><br><span class="line">	  	&#125;</span><br><span class="line">	&#125;<span class="keyword">catch</span>(...)</span><br><span class="line">	&#123;</span><br><span class="line">	  <span class="built_in">cout</span> &lt;&lt; <span class="string">"矩阵不正定"</span>;</span><br><span class="line">	  iter = <span class="number">0</span>;</span><br><span class="line">	  beta_in = beta_in0;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="string">"iteration number = "</span>;</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; iter;</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">	</span><br><span class="line">	mat Sigma_u = inv(X_in.col(p<span class="number">-1</span>).t()*invV*X_in.col(p<span class="number">-1</span>));</span><br><span class="line"></span><br><span class="line">  	<span class="keyword">return</span> Rcpp::List::create(Rcpp::Named(<span class="string">"iteration"</span>) = iter,</span><br><span class="line">		   Rcpp::Named(<span class="string">"betahat"</span>) = beta_in,</span><br><span class="line">		   Rcpp::Named(<span class="string">"covbetahat"</span>) = invI,</span><br><span class="line">		   Rcpp::Named(<span class="string">"=Sigma_u"</span>) = Sigma_u );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>#4. homogeneous approach </p>
<h2 id="Apply-homogeous-approach’s-code-on-evis71-dataset"><a href="#Apply-homogeous-approach’s-code-on-evis71-dataset" class="headerlink" title="Apply homogeous approach’s code on evis71 dataset"></a>Apply homogeous approach’s code on evis71 dataset</h2><h3 id="Results-obtained-using-heterogeous-approach"><a href="#Results-obtained-using-heterogeous-approach" class="headerlink" title="Results obtained using heterogeous approach:"></a>Results obtained using heterogeous approach:</h3><p><strong>Rcpp implementation</strong>:</p>
<p>50.298 sec elapsed</p>
<p>number of clusters: 6 </p>
<p>Estimated coefficients:</p>
<blockquote>
<p>para.fix<br>          [,1]<br>[1,] -1.063637<br>[2,] -0.123927<br>[3,]  3.142743<br>[4,]  2.664509<br>[5,]  2.300295<br>[6,]  1.947950<br>[7,]  1.753625<br>[8,]  1.435032</p>
</blockquote>
<p><strong>Original R code</strong> </p>
<p>158.233 sec elapsed</p>
<p>number of clusters = 6</p>
<p>Estimated coefficients:</p>
<blockquote>
<p>para.fix<br>          [,1]<br>[1,] -1.063637<br>[2,] -0.123927<br>[3,]  3.142743<br>[4,]  2.664509<br>[5,]  2.300295<br>[6,]  1.947950<br>[7,]  1.753625<br>[8,]  1.435032</p>
</blockquote>
<p>​     </p>
<h3 id="Results-obtained-using-homogenous-approach"><a href="#Results-obtained-using-homogenous-approach" class="headerlink" title="Results obtained using homogenous approach:"></a>Results obtained using homogenous approach:</h3><h5 id="Original-R-code"><a href="#Original-R-code" class="headerlink" title="Original R code"></a>Original R code</h5><p>43.5 sec elapsed</p>
<p>number of clusters = 14</p>
<h5 id="R-cpp-implementation"><a href="#R-cpp-implementation" class="headerlink" title="R cpp implementation"></a>R cpp implementation</h5><p>28.823 sec elapsed</p>
<p>number of clusters = 14</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/11/CS577_algorithm_notes/" rel="next" title="CS577 Introduction to algorithms notes">
                <i class="fa fa-chevron-left"></i> CS577 Introduction to algorithms notes
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/07/18/basic math copy/" rel="prev" title="Some math calculation tips">
                Some math calculation tips <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpg" alt="Kehui Yao">
          <p class="site-author-name" itemprop="name">Kehui Yao</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">32</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/KEHUIYAO" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/kehui-yao-a5b770165/" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/KehuiY" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Quasi-likelihood-functions"><span class="nav-number">1.</span> <span class="nav-text">1. Quasi-likelihood functions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Independent-observations"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Independent observations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-Covariance-functions"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 Covariance functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-Construction-of-the-quasi-likelihood-function"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2 Construction of the quasi-likelihood function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-1-Preliminary-knowledge-Fisher-information"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.1.2.1 Preliminary knowledge: Fisher information</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-2-Cramer-Rao-bound"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">1.1.2.2 Cramer-Rao bound</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Scalar-case"><span class="nav-number">1.1.2.2.1.</span> <span class="nav-text">Scalar case</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multivariate-case"><span class="nav-number">1.1.2.2.2.</span> <span class="nav-text">Multivariate case</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Single-parameter-proof"><span class="nav-number">1.1.2.2.3.</span> <span class="nav-text">Single-parameter proof</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-3-First-single-component-of-vector-Y"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">1.1.2.3 First single component of vector Y</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-4-Sum-of-the-individual-contributions"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">1.1.2.4 Sum of the individual contributions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-3-Quasi-deviance-function"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.1.3 Quasi-deviance function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-4-Parameter-estimation"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.1.4 Parameter estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-4-1-Newton-Raphson-iteration-of-estimating-beta"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">1.1.4.1 Newton-Raphson iteration of estimating $\beta$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-4-2-Details-of-newton-raphson-iteration-used-on-the-method-of-scoring"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">1.1.4.2 Details of newton-raphson iteration used on the method of scoring</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-4-3-Estimation-of-sigma-2"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">1.1.4.3 Estimation of $\sigma^{2}$</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Dependent-observations"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Dependent observations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-Quasi-likelihood-estimating-equations"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 Quasi-likelihood estimating equations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-论文基本思路："><span class="nav-number">2.</span> <span class="nav-text">2. 论文基本思路：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Candidate-model"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">Candidate model:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-and-Estimation"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">Model and Estimation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Determine-which-candidate-model-is-the-best"><span class="nav-number">2.0.0.3.</span> <span class="nav-text">Determine which candidate model is the best</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Three-step-algorithm-to-wrap-it-up"><span class="nav-number">2.0.0.4.</span> <span class="nav-text">Three-step algorithm to wrap it up</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Identification-of-multiple-clusters"><span class="nav-number">2.0.0.5.</span> <span class="nav-text">Identification of multiple clusters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simulation-based-on-a-hierarchical-generalized-linear-model"><span class="nav-number">2.0.0.6.</span> <span class="nav-text">Simulation based on a hierarchical generalized linear model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#具体代码实现中的技巧："><span class="nav-number">2.0.0.7.</span> <span class="nav-text">具体代码实现中的技巧：</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Rcpp-基础知识"><span class="nav-number">3.</span> <span class="nav-text">3. Rcpp 基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Rcpp-Armadillo-example-code-这是quasi-likelihood-function的c-inplementation"><span class="nav-number">3.1.</span> <span class="nav-text">Rcpp Armadillo example code: (这是quasi likelihood function的c++ inplementation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Apply-homogeous-approach’s-code-on-evis71-dataset"><span class="nav-number">3.2.</span> <span class="nav-text">Apply homogeous approach’s code on evis71 dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Results-obtained-using-heterogeous-approach"><span class="nav-number">3.2.1.</span> <span class="nav-text">Results obtained using heterogeous approach:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Results-obtained-using-homogenous-approach"><span class="nav-number">3.2.2.</span> <span class="nav-text">Results obtained using homogenous approach:</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Original-R-code"><span class="nav-number">3.2.2.0.1.</span> <span class="nav-text">Original R code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#R-cpp-implementation"><span class="nav-number">3.2.2.0.2.</span> <span class="nav-text">R cpp implementation</span></a></li></ol></li></ol></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kehui Yao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
