<!doctype html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="Boosting introduction这篇文章部分内容转自 Materials for an online-course - “Practical XGBoost in Python”。  What is the idea of boosting Why use tree as a weak classifier What are some common boosting implementa">
<meta property="og:type" content="article">
<meta property="og:title" content="xgboost principle and practice">
<meta property="og:url" content="http://yoursite.com/2019/03/21/xgboost/index.html">
<meta property="og:site_name" content="Kehui&#39;s Blog">
<meta property="og:description" content="Boosting introduction这篇文章部分内容转自 Materials for an online-course - “Practical XGBoost in Python”。  What is the idea of boosting Why use tree as a weak classifier What are some common boosting implementa">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/03/21/image/output_15_0.png">
<meta property="og:image" content="http://yoursite.com/2019/03/21/image/output_21_0.png">
<meta property="og:image" content="http://yoursite.com/2019/03/21/image/output_29_0.png">
<meta property="og:updated_time" content="2019-03-22T02:09:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xgboost principle and practice">
<meta name="twitter:description" content="Boosting introduction这篇文章部分内容转自 Materials for an online-course - “Practical XGBoost in Python”。  What is the idea of boosting Why use tree as a weak classifier What are some common boosting implementa">
<meta name="twitter:image" content="http://yoursite.com/2019/03/21/image/output_15_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"remove","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/21/xgboost/">





  <title> xgboost principle and practice | Kehui's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kehui's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Do statistics, learn some computer science.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/21/xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kehui Yao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kehui's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                xgboost principle and practice
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-21T00:29:15-05:00">
                2019-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Boosting-introduction"><a href="#Boosting-introduction" class="headerlink" title="Boosting introduction"></a>Boosting introduction</h2><p>这篇文章部分内容转自 Materials for an online-course - “Practical XGBoost in Python”。</p>
<ul>
<li><a href="#idea-of-boosting">What is the idea of boosting</a></li>
<li><a href="#weak-classifier">Why use tree as a weak classifier</a></li>
<li><a href="#common-algorithms">What are some common boosting implementations</a></li>
<li><a href="#how-xgboost-helps">How XGBoost helps</a></li>
<li><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">Tianqi chen’s slide</a></li>
<li><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">xgboost original paper</a></li>
</ul>
<a id="more"></a>
<h3 id="Idea-of-boosting"><a href="#Idea-of-boosting" class="headerlink" title="Idea of boosting "></a>Idea of boosting <a name="idea-of-boosting"></a></h3><p>Let’s start with intuitive definition of the concept:</p>
<blockquote>
<p><strong>Boosting</strong> (<em>Freud and Shapire, 1996</em>) - algorithm allowing to fit <strong>many</strong> weak classifiers to <strong>reweighted</strong> versions of the training data. Classify final examples by majority voting.</p>
</blockquote>
<p>When using boosting techinque all instance in dataset are assigned a score that tells <em>how difficult to classify</em> they are. In each following iteration the algorithm pays more attention (assign bigger weights) to instances that were wrongly classified previously.</p>
<p>In the first iteration all instance weights are equal.</p>
<p>Ensemble parameters are optimized in <strong>stagewise way</strong> which means that we are calculating optimal parameters for the next classifier holding fixed what was already calculated. This might sound like a limitation but turns out it’s a very resonable way of regularizing the model.</p>
<h3 id="Weak-classifier-why-tree"><a href="#Weak-classifier-why-tree" class="headerlink" title="Weak classifier - why tree? "></a>Weak classifier - why tree? <a name="weak-classifier"></a></h3><p>First what is a weak classifier?</p>
<blockquote>
<p><strong>Weak classifier</strong> - an algorithm <strong>slightly better</strong> than random guessing.</p>
</blockquote>
<p>Every algorithm can be used as a base for boosting techinique, but trees have some nice properties that makes them more suitable candidates.</p>
<h4 id="Pro’s"><a href="#Pro’s" class="headerlink" title="Pro’s"></a>Pro’s</h4><ul>
<li>computational scalability,</li>
<li>handling missing values,</li>
<li>robust to outliers,</li>
<li>does not require feature scalling,</li>
<li>can deal with irrelevant inputs,</li>
<li>interpretable (if small),</li>
<li>can handle mixed predictors (quantitive and qualitative)</li>
</ul>
<h4 id="Con’s"><a href="#Con’s" class="headerlink" title="Con’s"></a>Con’s</h4><ul>
<li>can’t extract linear combination of features</li>
<li>small predictive power (high variance)</li>
</ul>
<p>Boosting techinque can try to reduce the variance by <strong>averaging</strong> many <strong>different</strong> trees (where each one is solving the same problem)</p>
<h3 id="Common-Algorithms-warning-MATH-INCLUDED"><a href="#Common-Algorithms-warning-MATH-INCLUDED" class="headerlink" title="Common Algorithms (warning MATH INCLUDED) "></a>Common Algorithms (warning MATH INCLUDED) <a name="common-algorithms"></a></h3><p>In every machine learning model the training objective is a sum of a loss function $L$ and regularization $\Omega$:</p>
<p>$$<br>obj = L + \Omega<br>$$</p>
<p>The loss function controls the predictive power of an algorithm and regularization term controls it’s simplicity.</p>
<h4 id="AdaBoost-Adaptive-Boosting"><a href="#AdaBoost-Adaptive-Boosting" class="headerlink" title="AdaBoost (Adaptive Boosting)"></a>AdaBoost (Adaptive Boosting)</h4><p>The implementation of boosting technique using decision tress (it’s a <em>meta-estimator</em> which means you can fit any classifier in). The intuitive recipie is presented below:</p>
<p><strong>Algorithm</strong>:</p>
<p>Assume that the number of training samples is denoted by $N$, and the number of iterations (created trees) is $M$. Notice that possible class outputs are $Y={-1,1}$</p>
<ol>
<li>Initialize the observation weights $w_i=\frac{1}{N}$ where $i = 1,2, \dots, N$</li>
<li>For $m=1$ to $M$:<ul>
<li>fit a classifier $G_m(x)$ to the training data using weights $w_i$,</li>
<li>compute $err_m = \frac{\sum_{i=1}^{N} w_i I (y_i \neq G_m(x))}{\sum_{i=1}^{N}w_i}$,</li>
<li>compute $\alpha_m = \log ((1-err_m)/err_m)$,</li>
<li>set $w_i \leftarrow w_i \cdot \exp [\alpha_m \cdot I (y_i \neq G_m(x)]$, where $i = 1,2, \dots, N$</li>
</ul>
</li>
<li>Output $G_m(x) = sign [\sum_{m=1}^{M} \alpha_m G_m(x)]$</li>
</ol>
<h4 id="Generalized-Boosted-Models"><a href="#Generalized-Boosted-Models" class="headerlink" title="Generalized Boosted Models"></a>Generalized Boosted Models</h4><p>We can take advantage of the fact that the loss function can be represented with a form suitable for optimalization (due to the stage-wise additivity). This creates a class of general boosting algorithms named simply <strong>generalized boosted model (GBM)</strong>.</p>
<p>An example of a GBM is <strong>Gradient Boosted Tree</strong> which uses decision tree as an estimator. It can work with different loss functions (regression, classification, risk modeling etc.), evaluate it’s  gradient and approximates it with a simple tree (stage-wisely, that minimizes the overall error).</p>
<p>AdaBoost is a special case of Gradient Boosted Tree that uses exponential loss function. You can learn more about GBM in this <a href="https://www.youtube.com/watch?v=wPqtzj5VZus&amp;feature=youtu.be" target="_blank" rel="noopener">video</a>.</p>
<h3 id="How-XGBoost-helps"><a href="#How-XGBoost-helps" class="headerlink" title="How XGBoost helps "></a>How XGBoost helps <a name="how-xgboost-helps"></a></h3><p>The problem with most tree packages is that they don’t take regularization issues very seriously - they allow to grow many very similar trees that can be also sometimes quite bushy.</p>
<p>GBT tries to approach this problem by adding some regularization parameters. We can:</p>
<ul>
<li>control tree structure (maximum depth, minimum samples per leaf),</li>
<li>control learning rate (shrinkage),</li>
<li>reduce variance by introducing randomness (stochastic gradient boosting - using random subsamples of instances and features)</li>
</ul>
<p>But it could be improved even further. Enter XGBoost.</p>
<blockquote>
<p><strong>XGBoost</strong> (<em>extreme gradient boosting</em>) is a <strong>more regularized</strong> version of Gradient Boosted Trees.</p>
</blockquote>
<p>It was develop by Tianqi Chen in C++ but also enables interfaces for Python, R, Julia. Used for supervised learning problem gave win to <a href="https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions" target="_blank" rel="noopener">many Kaggle competitions</a>.</p>
<p>The main advantages:</p>
<ul>
<li>good bias-variance (simple-predictive) trade-off “out of the box”,</li>
<li>great computation speed,</li>
<li>package is evolving (author is willing to accept many PR from community)</li>
</ul>
<p>XGBoost’s objective function is a sum of a specific loss function evaluated over all predictions and a sum of regularization term for all predictors ($K$ trees). In the formula $f_k$ means a prediction coming from k-th tree.</p>
<p>$$<br>obj(\theta) = \sum_{i}^{n} l(y_i - \hat{y_i}) +  \sum_{k=1}^{K} \Omega (f_k)<br>$$</p>
<p>Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:</p>
<p>$$<br>\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T}w_j^2<br>$$</p>
<p>First part ($\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\frac{1}{2} \lambda \sum_{j=1}^{T}w_j^2$) watches over the their’s scores.</p>
<p>To optimize the objective a gradient descent is used, this leads to a problem of finding an optimal structure of the successive tree. More mathematics about the algorithm is not included in the scope of this course, but pretty decent informations can be found on the package <a href="http://xgboost.readthedocs.io/" target="_blank" rel="noopener">docs page</a> and in <a href="http://www.slideshare.net/ShangxuanZhang/xgboost" target="_blank" rel="noopener">this</a> presentation.</p>
<h2 id="Boosting-practice"><a href="#Boosting-practice" class="headerlink" title="Boosting practice"></a>Boosting practice</h2><p><strong>This chapter includes</strong>:</p>
<ul>
<li><a href="#data-preparation">Data preparation</a></li>
<li><a href="#sdt">Using single decision tree</a></li>
<li><a href="#ada">Boosting with AdaBoost</a></li>
<li><a href="#gbt">Gradient Boosted Trees - why not?</a></li>
</ul>
<p>Now let’s time to see how boosting is applied in practice. Hopefully the <code>scikit-learn</code> package provides all described packages. Begin with importing all required libraries. XGBoost package will be described more in later lectures.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss, accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># classifiers</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># reproducibility</span></span><br><span class="line">seed = <span class="number">104</span></span><br></pre></td></tr></table></figure>
<h3 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data "></a>Prepare data <a name="data-preparation"></a></h3><p>In all examples we will be dealing with <strong>binary classification</strong>.  Generate 20 dimensional artificial dataset with 1000 samples, where 8 features holding information, 3 are redundant and 2 repeated.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">8</span>, n_redundant=<span class="number">3</span>, n_repeated=<span class="number">2</span>, random_state=seed)</span><br></pre></td></tr></table></figure>
<p>And finally perform a split into train/test parts. It will be useful for validating the performance of all methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=seed)</span><br></pre></td></tr></table></figure>
<p>All algorithms won’t be tuned at this point. A sensible set of default settings will be applied, making the whole things less complicated.</p>
<blockquote>
<p><a href="https://www.kaggle.com/wiki/LogarithmicLoss" target="_blank" rel="noopener"><em>Logarithmic loss</em></a> and accuracy were chosen to evaluate the results. It’s also important to remeber about reproducibility - you should always set all <code>seed</code> parameters to the same value.</p>
</blockquote>
<p>Let’s perform a target variable distribution sanity check before digging into algorithms.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train label distribution:"</span>)</span><br><span class="line">print(Counter(y_train))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest label distribution:"</span>)</span><br><span class="line">print(Counter(y_test))</span><br></pre></td></tr></table></figure>
<pre><code>Train label distribution:
Counter({1: 405, 0: 395})

Test label distribution:
Counter({0: 104, 1: 96})
</code></pre><p>Target variable is equally distribued across both dataset.</p>
<h3 id="Single-Decision-Tree"><a href="#Single-Decision-Tree" class="headerlink" title="Single Decision Tree "></a>Single Decision Tree <a name="sdt"></a></h3><p>The following code will create a single decision tree, fit it using training data and evaluate the results using test sample.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">decision_tree = DecisionTreeClassifier(random_state=seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train classifier</span></span><br><span class="line">decision_tree.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict output</span></span><br><span class="line">decision_tree_y_pred  = decision_tree.predict(X_test)</span><br><span class="line">decision_tree_y_pred_prob  = decision_tree.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluation</span></span><br><span class="line">decision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)</span><br><span class="line">decision_tree_logloss = log_loss(y_test, decision_tree_y_pred_prob)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"== Decision Tree =="</span>)</span><br><span class="line">print(<span class="string">"Accuracy: &#123;0:.2f&#125;"</span>.format(decision_tree_accuracy))</span><br><span class="line">print(<span class="string">"Log loss: &#123;0:.2f&#125;"</span>.format(decision_tree_logloss))</span><br><span class="line">print(<span class="string">"Number of nodes created: &#123;&#125;"</span>.format(decision_tree.tree_.node_count))</span><br></pre></td></tr></table></figure>
<pre><code>== Decision Tree ==
Accuracy: 0.78
Log loss: 7.60
Number of nodes created: 167
</code></pre><p>We can see two things:</p>
<ol>
<li>the log loss score is not very promising (due to the fact that leaves in decision tree outputs either <code>0</code> or <code>1</code> as probability which is heaviliy penalized in case of errors, but the accuracy score is quite decent,</li>
<li>the tree is complicated (large number of nodes)</li>
</ol>
<p>You can inspect first few predicted outputs, and see that only 2 instances out of 5 were classified correctly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'True labels:'</span>)</span><br><span class="line">print(y_test[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted labels:'</span>)</span><br><span class="line">print(decision_tree_y_pred[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted probabilities:'</span>)</span><br><span class="line">print(decision_tree_y_pred_prob[:<span class="number">5</span>,])</span><br></pre></td></tr></table></figure>
<pre><code>True labels:
[1 1 1 0 1]

Predicted labels:
[0 0 1 0 0]

Predicted probabilities:
[[ 1.  0.]
 [ 1.  0.]
 [ 0.  1.]
 [ 1.  0.]
 [ 1.  0.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dt_viz_file = <span class="string">'../images/dt.dot'</span></span><br><span class="line">dt_png_file = <span class="string">'../images/dt.png'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create visualization</span></span><br><span class="line">export_graphviz(decision_tree, out_file=dt_viz_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PNG</span></span><br><span class="line">command = [<span class="string">"dot"</span>, <span class="string">"-Tpng"</span>, dt_viz_file, <span class="string">"-o"</span>, dt_png_file]</span><br><span class="line">subprocess.check_call(command)</span><br><span class="line"></span><br><span class="line"><span class="comment"># display image</span></span><br><span class="line">Image(filename=dt_png_file)</span><br></pre></td></tr></table></figure>
<p><img src="../image/output_15_0.png" alt="png"></p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost "></a>AdaBoost <a name="ada"></a></h3><p>In the example below we are creating a AdaBoost classifier running on 1000 iterations (1000 trees created). Also we are growing decision node up to first split (they are called <em>decision stumps</em>). We are also going to use <code>SAMME</code> algorithm which is inteneded to work with discrete data (output from <code>base_estimator</code> is <code>0</code> or <code>1</code>). Please refer to the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" target="_blank" rel="noopener">documentation</a> and <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html" target="_blank" rel="noopener">here</a> for more details.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">adaboost = AdaBoostClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(max_depth=<span class="number">1</span>),</span><br><span class="line">    algorithm=<span class="string">'SAMME'</span>,</span><br><span class="line">    n_estimators=<span class="number">1000</span>,</span><br><span class="line">    random_state=seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train classifier</span></span><br><span class="line">adaboost.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate predictions</span></span><br><span class="line">adaboost_y_pred = adaboost.predict(X_test)</span><br><span class="line">adaboost_y_pred_prob = adaboost.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">adaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)</span><br><span class="line">adaboost_logloss = log_loss(y_test, adaboost_y_pred_prob)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"== AdaBoost =="</span>)</span><br><span class="line">print(<span class="string">"Accuracy: &#123;0:.2f&#125;"</span>.format(adaboost_accuracy))</span><br><span class="line">print(<span class="string">"Log loss: &#123;0:.2f&#125;"</span>.format(adaboost_logloss))</span><br></pre></td></tr></table></figure>
<pre><code>== AdaBoost ==
Accuracy: 0.78
Log loss: 0.69
</code></pre><p>The log-loss metrics is much lower than in single decision tree (mainly to the fact that now we obtain probabilities output). The accuracy is the same, but notice that the structure of the tree is much simpler. We are creating 1000 <strong>decision tree stumps</strong>.</p>
<p>Also here a quick peek into predicted values show that now 4 out of 5 first test instances are classified correctly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'True labels:'</span>)</span><br><span class="line">print(y_test[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted labels:'</span>)</span><br><span class="line">print(adaboost_y_pred[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted probabilities:'</span>)</span><br><span class="line">print(adaboost_y_pred_prob[:<span class="number">5</span>,])</span><br></pre></td></tr></table></figure>
<pre><code>True labels:
[1 1 1 0 1]

Predicted labels:
[1 0 1 0 1]

Predicted probabilities:
[[ 0.50211294  0.49788706]
 [ 0.5021334   0.4978666 ]
 [ 0.50162035  0.49837965]
 [ 0.51639587  0.48360413]
 [ 0.49926165  0.50073835]]
</code></pre><p>Just for clarity, let’s check how the first tree looks like.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ada_t1 = adaboost.estimators_[<span class="number">0</span>]</span><br><span class="line">ada_t1_viz_file = <span class="string">'../images/ada-t1.dot'</span></span><br><span class="line">ada_t1_png_file = <span class="string">'../images/ada-t1.png'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create visualization</span></span><br><span class="line">export_graphviz(ada_t1, out_file=ada_t1_viz_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PNG</span></span><br><span class="line">command = [<span class="string">"dot"</span>, <span class="string">"-Tpng"</span>, ada_t1_viz_file, <span class="string">"-o"</span>, ada_t1_png_file]</span><br><span class="line">subprocess.check_call(command)</span><br><span class="line"></span><br><span class="line"><span class="comment"># display image</span></span><br><span class="line">Image(filename=ada_t1_png_file)</span><br></pre></td></tr></table></figure>
<p><img src="../image/output_21_0.png" alt="png"></p>
<p>What’s it’s error and contribution into final ensemble</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Error: &#123;0:.2f&#125;"</span>.format(adaboost.estimator_errors_[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Tree importance: &#123;0:.2f&#125;"</span>.format(adaboost.estimator_weights_[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Error: 0.33
Tree importance: 0.71
</code></pre><h3 id="Gradient-Boosted-Trees"><a href="#Gradient-Boosted-Trees" class="headerlink" title="Gradient Boosted Trees"></a>Gradient Boosted Trees<a name="gbt"></a></h3><p>Let’s construct a gradient boosted tree consiting of 1000 trees where each successive one will be created with gradient optimization. Again we are going to leave most parameters with their default values, specifiy only maximum depth of the tree to 1 (again decision stumps), and setting warm start for more intelligent computations. Please refer to the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" target="_blank" rel="noopener">docs</a> if something is not clear.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">gbc = GradientBoostingClassifier(</span><br><span class="line">    max_depth=<span class="number">1</span>,</span><br><span class="line">    n_estimators=<span class="number">1000</span>,</span><br><span class="line">    warm_start=<span class="literal">True</span>,</span><br><span class="line">    random_state=seed)</span><br><span class="line">gbc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make predictions</span></span><br><span class="line">gbc_y_pred = gbc.predict(X_test)</span><br><span class="line">gbc_y_pred_prob = gbc.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate log loss</span></span><br><span class="line">gbc_accuracy = accuracy_score(y_test, gbc_y_pred)</span><br><span class="line">gbc_logloss = log_loss(y_test, gbc_y_pred_prob)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"== Gradient Boosting =="</span>)</span><br><span class="line">print(<span class="string">"Accuracy: &#123;0:.2f&#125;"</span>.format(gbc_accuracy))</span><br><span class="line">print(<span class="string">"Log loss: &#123;0:.2f&#125;"</span>.format(gbc_logloss))</span><br></pre></td></tr></table></figure>
<pre><code>== Gradient Boosting ==
Accuracy: 0.81
Log loss: 0.48
</code></pre><p>The obtained results are obviously the best of all presented algorithm. We have obtained most accurate algorithm giving more sensible predictions about class probabilities.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'True labels:'</span>)</span><br><span class="line">print(y_test[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted labels:'</span>)</span><br><span class="line">print(gbc_y_pred[:<span class="number">5</span>,])</span><br><span class="line">print(<span class="string">'\nPredicted probabilities:'</span>)</span><br><span class="line">print(gbc_y_pred_prob[:<span class="number">5</span>,])</span><br></pre></td></tr></table></figure>
<pre><code>True labels:
[1 1 1 0 1]

Predicted labels:
[1 0 1 0 1]

Predicted probabilities:
[[ 0.3698346   0.6301654 ]
 [ 0.77521996  0.22478004]
 [ 0.45077105  0.54922895]
 [ 0.9660623   0.0339377 ]
 [ 0.31039421  0.68960579]]
</code></pre><p>The difference is that GBC uses <code>DecisionTreeRegressor</code> classifier as the estimator with <em>mean-square error</em> as criterion. This results of slightly different output of the tree - now the leaf contains a predicted value (while the first splitting point remains the same).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gbc_t1 = gbc.estimators_[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line">gbc_t1_viz_file = <span class="string">'../images/gbc-t1.dot'</span></span><br><span class="line">gbc_t1_png_file = <span class="string">'../images/gbc-t1.png'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create visualization</span></span><br><span class="line">export_graphviz(gbc_t1, out_file=gbc_t1_viz_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PNG</span></span><br><span class="line">command = [<span class="string">"dot"</span>, <span class="string">"-Tpng"</span>, gbc_t1_viz_file, <span class="string">"-o"</span>, gbc_t1_png_file]</span><br><span class="line">subprocess.check_call(command)</span><br><span class="line"></span><br><span class="line"><span class="comment"># display image</span></span><br><span class="line">Image(filename=gbc_t1_png_file)</span><br></pre></td></tr></table></figure>
<p><img src="../image/output_29_0.png" alt="png"></p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h4 id="Using-standard-interface"><a href="#Using-standard-interface" class="headerlink" title="Using standard interface"></a>Using standard interface</h4><p>The following notebooks presents the basic usage of native XGBoost Python interface.</p>
<p><strong>Flight-plan</strong>:</p>
<ul>
<li><a href="#libs">load libraries</a> and <a href="#data">prepare data</a>,</li>
<li><a href="#params">specify parameters</a>,</li>
<li><a href="#train">train classifier</a>,</li>
<li><a href="#predict">make predictions</a></li>
</ul>
<h4 id="Loading-libraries"><a href="#Loading-libraries" class="headerlink" title="Loading libraries"></a>Loading libraries<a name="libs"></a></h4><p>Begin with loading all required libraries in one place:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br></pre></td></tr></table></figure>
<h4 id="Loading-data"><a href="#Loading-data" class="headerlink" title="Loading data"></a>Loading data<a name="data"></a></h4><p>We are going to use bundled <a href="https://archive.ics.uci.edu/ml/datasets/Mushroom" target="_blank" rel="noopener">Agaricus</a> dataset which can be downloaded <a href="https://github.com/dmlc/xgboost/tree/master/demo/data" target="_blank" rel="noopener">here</a>.</p>
<blockquote>
<p>This data set records biological attributes of different mushroom species, and the target is to predict whether it is poisonous</p>
</blockquote>
<blockquote>
<p>This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom;</p>
</blockquote>
<p>It consist of 8124 instances, characterized by 22 attributes (both numeric and categorical). The target class is either 0 or 1 which means binary classification problem.</p>
<blockquote>
<p><strong>Important</strong>: XGBoost handles only numeric variables.</p>
</blockquote>
<p>Lucily all the data have alreay been pre-process for us. Categorical variables have been encoded, and all instances divided into train and test datasets. You will know how to do this on your own in later lectures.</p>
<p>Data needs to be stored in <code>DMatrix</code> object which is designed to handle sparse datasets. It can be populated in couple ways:</p>
<ul>
<li>using libsvm format txt file,</li>
<li>using Numpy 2D array (most popular),</li>
<li>using XGBoost binary buffer file</li>
</ul>
<p>In this case we’ll use first option.</p>
<blockquote>
<p>Libsvm files stores only non-zero elements in format</p>
<p><code>&lt;label&gt; &lt;feature_a&gt;:&lt;value_a&gt; &lt;feature_c&gt;:&lt;value_c&gt; ... &lt;feature_z&gt;:&lt;value_z&gt;</code></p>
<p>Any missing features indicate that it’s corresponding value is 0.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(<span class="string">'../data/agaricus.txt.train'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'../data/agaricus.txt.test'</span>)</span><br></pre></td></tr></table></figure>
<p>Let’s examine what was loaded:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train dataset contains &#123;0&#125; rows and &#123;1&#125; columns"</span>.format(dtrain.num_row(), dtrain.num_col()))</span><br><span class="line">print(<span class="string">"Test dataset contains &#123;0&#125; rows and &#123;1&#125; columns"</span>.format(dtest.num_row(), dtest.num_col()))</span><br></pre></td></tr></table></figure>
<pre><code>Train dataset contains 6513 rows and 127 columns
Test dataset contains 1611 rows and 127 columns
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train possible labels: "</span>)</span><br><span class="line">print(np.unique(dtrain.get_label()))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest possible labels: "</span>)</span><br><span class="line">print(np.unique(dtest.get_label()))</span><br></pre></td></tr></table></figure>
<pre><code>Train possible labels:
[ 0.  1.]

Test possible labels:
[ 0.  1.]
</code></pre><h4 id="Specify-training-parameters"><a href="#Specify-training-parameters" class="headerlink" title="Specify training parameters"></a>Specify training parameters<a name="params"></a></h4><p>Let’s make the following assuptions and adjust algorithm parameters to it:</p>
<ul>
<li>we are dealing with binary classification problem (<code>&#39;objective&#39;:&#39;binary:logistic&#39;</code>),</li>
<li>we want shallow single trees with no more than 2 levels (<code>&#39;max_depth&#39;:2</code>),</li>
<li>we don’t any oupout (<code>&#39;silent&#39;:1</code>),</li>
<li>we want algorithm to learn fast and aggressively (<code>&#39;eta&#39;:1</code>),</li>
<li>we want to iterate only 5 rounds</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">2</span>,</span><br><span class="line">    <span class="string">'silent'</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">'eta'</span>:<span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">num_rounds = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h4 id="Training-classifier"><a href="#Training-classifier" class="headerlink" title="Training classifier"></a>Training classifier<a name="train"></a></h4><p>To train the classifier we simply pass to it a training dataset, parameters list and information about number of iterations.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst = xgb.train(params, dtrain, num_rounds)</span><br></pre></td></tr></table></figure>
<p>We can also observe performance on test dataset using <code>watchlist</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">watchlist  = [(dtest,<span class="string">'test'</span>), (dtrain,<span class="string">'train'</span>)] <span class="comment"># native interface only</span></span><br><span class="line">bst = xgb.train(params, dtrain, num_rounds, watchlist)</span><br></pre></td></tr></table></figure>
<pre><code>[0]    test-error:0.042831    train-error:0.046522
[1]    test-error:0.021726    train-error:0.022263
[2]    test-error:0.006207    train-error:0.007063
[3]    test-error:0.018001    train-error:0.0152
[4]    test-error:0.006207    train-error:0.007063
</code></pre><h3 id="Make-predictions"><a href="#Make-predictions" class="headerlink" title="Make predictions"></a>Make predictions<a name="predict"></a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds_prob = bst.predict(dtest)</span><br><span class="line">preds_prob</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.08073306,  0.92217326,  0.08073306, ...,  0.98059034,
        0.01182149,  0.98059034], dtype=float32)
</code></pre><p>Calculate simple accuracy metric to verify the results. Of course validation should be performed accordingly to the dataset, but in this case accuracy is sufficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">labels = dtest.get_label()</span><br><span class="line">preds = preds_prob &gt; <span class="number">0.5</span> <span class="comment"># threshold</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(preds)):</span><br><span class="line">    <span class="keyword">if</span> (labels[i] == preds[i]):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted correctly: &#123;0&#125;/&#123;1&#125;'</span>.format(correct, len(preds)))</span><br><span class="line">print(<span class="string">'Error: &#123;0:.4f&#125;'</span>.format(<span class="number">1</span>-correct/len(preds)))</span><br></pre></td></tr></table></figure>
<pre><code>Predicted correctly: 1601/1611
Error: 0.0062
</code></pre><h3 id="Using-Scikit-learn-Interface"><a href="#Using-Scikit-learn-Interface" class="headerlink" title="Using Scikit-learn Interface"></a>Using Scikit-learn Interface</h3><p>The following notebook presents the alternative approach for using XGBoost algorithm.</p>
<p><strong>What’s included</strong>:</p>
<ul>
<li><a href="#libs">load libraries</a> and <a href="#data">prepare data</a>,</li>
<li><a href="#params">specify parameters</a>,</li>
<li><a href="#train">train classifier</a>,</li>
<li><a href="#predict">make predictions</a></li>
</ul>
<h4 id="Loading-libraries-1"><a href="#Loading-libraries-1" class="headerlink" title="Loading libraries"></a>Loading libraries<a name="libs"></a></h4><p>Begin with loading all required libraries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_files</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBClassifier</span><br></pre></td></tr></table></figure>
<h4 id="Loading-data-1"><a href="#Loading-data-1" class="headerlink" title="Loading data"></a>Loading data<a name="data"></a></h4><p>We are going to use the same dataset as in previous lecture. The scikit-learn package provides a convenient function <code>load_svmlight</code> capable of reading many libsvm files at once and storing them as Scipy’s sparse matrices.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, y_train, X_test, y_test = load_svmlight_files((<span class="string">'../data/agaricus.txt.train'</span>, <span class="string">'../data/agaricus.txt.test'</span>))</span><br></pre></td></tr></table></figure>
<p>Examine what was loaded</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train dataset contains &#123;0&#125; rows and &#123;1&#125; columns"</span>.format(X_train.shape[<span class="number">0</span>], X_train.shape[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">"Test dataset contains &#123;0&#125; rows and &#123;1&#125; columns"</span>.format(X_test.shape[<span class="number">0</span>], X_test.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Train dataset contains 6513 rows and 126 columns
Test dataset contains 1611 rows and 126 columns
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Train possible labels: "</span>)</span><br><span class="line">print(np.unique(y_train))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest possible labels: "</span>)</span><br><span class="line">print(np.unique(y_test))</span><br></pre></td></tr></table></figure>
<pre><code>Train possible labels:
[ 0.  1.]

Test possible labels:
[ 0.  1.]
</code></pre><h4 id="Specify-training-parameters-1"><a href="#Specify-training-parameters-1" class="headerlink" title="Specify training parameters"></a>Specify training parameters<a name="params"></a></h4><p>All the parameters are set like in the previous example</p>
<ul>
<li>we are dealing with binary classification problem (<code>&#39;objective&#39;:&#39;binary:logistic&#39;</code>),</li>
<li>we want shallow single trees with no more than 2 levels (<code>&#39;max_depth&#39;:2</code>),</li>
<li>we don’t any oupout (<code>&#39;silent&#39;:1</code>),</li>
<li>we want algorithm to learn fast and aggressively (<code>&#39;learning_rate&#39;:1</code>), (in naive named <code>eta</code>)</li>
<li>we want to iterate only 5 rounds (<code>n_estimators</code>)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">'n_estimators'</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Training-classifier-1"><a href="#Training-classifier-1" class="headerlink" title="Training classifier"></a>Training classifier<a name="train"></a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst = XGBClassifier(**params).fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h4 id="Make-predictions-1"><a href="#Make-predictions-1" class="headerlink" title="Make predictions"></a>Make predictions<a name="predict"></a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = bst.predict(X_test)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.,  1.,  0., ...,  1.,  0.,  1.])
</code></pre><p>Calculate obtained error</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(preds)):</span><br><span class="line">    <span class="keyword">if</span> (y_test[i] == preds[i]):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">acc = accuracy_score(y_test, preds)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted correctly: &#123;0&#125;/&#123;1&#125;'</span>.format(correct, len(preds)))</span><br><span class="line">print(<span class="string">'Error: &#123;0:.4f&#125;'</span>.format(<span class="number">1</span>-acc))</span><br></pre></td></tr></table></figure>
<pre><code>Predicted correctly: 1601/1611
Error: 0.0062
</code></pre>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/19/factorization-machine/" rel="next" title="Factorization machine principle and projects">
                <i class="fa fa-chevron-left"></i> Factorization machine principle and projects
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/21/xgboost-tuning-parameters/" rel="prev" title="xgboost_tuning_parameters">
                xgboost_tuning_parameters <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kehui Yao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
